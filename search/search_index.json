{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Alma CookBook","text":""},{"location":"#a-collection-of-recipes-for-scientific-computing-at-the-icr","title":"A collection of recipes for scientific computing at the ICR.","text":"<p>These recipes are primarily targeted at computing with Alma, the ICR's HPC system, but also cover other software and tools that are commonly used in scientific computing.</p> <p>Please browse the menu for recipes and patterns and tested ideas for software development and scientific computing on Alma. Currently, there are recipes for Nextflow, GPU programming, MatLab from python and conda and mamba environments for python and R.  </p> <p>Note this site is always under active construction and is updated with content as it is delivered. </p> <p>Maintained by the Research Software Engineering team in Scientific Computing at the ICR. Contact Service Hub and ask for the RSE team if you have any questions or suggestions.  </p>"},{"location":"about/","title":"About and Help","text":"<p>Alma and the applications and software that runs on it, is maintained by Scientific Computing, with custom software development assistance available from the RSE Group. For any help or questions email scientific computing.</p>"},{"location":"first_steps/","title":"First Steps to using Alma","text":"<p>The objective of this section is to briefly present the different software tools which are made available on Alma HPC. This includes: code versioning tools, Alma GUI version or AlmaOnDemand, package manager tools, among others.</p> <p>It is recommended to set up carefully these tools prior to running scripts on Alma.  This will ensure that the majority of the further help tutorials are accurate for your environment.</p> <p>If you don't think any of this is going to be necessary, feel free to skip it.  </p>"},{"location":"first_steps/#1-setting-up-ssh-key-for-alma-credits-to-vscode-and-blissweb-on-stackoverflow","title":"1. Setting up ssh key for Alma (credits to VSCode and blissweb on StackOverflow)","text":"<p>In order to seamlessly log into Alma without having to type in your password, you need to set up an ssh key. Here are the steps to do so:</p> <ol> <li>Generate public and a private key</li> </ol> <p>Mac/Linux/WSL: <pre><code># command line\nssh-keygen -t ed25519 -f ~/.ssh/id_ed25519-remote-ssh\n</code></pre></p> <p>Windows: <pre><code># command line\nssh-keygen -t ed25519 -f \"$HOME\\.ssh\\id_ed25519-remote-ssh\"\n</code></pre></p> <p>IMPORTANT! Two keys will be generated:</p> <p>id_ed25519-remote-ssh        (private key)</p> <p>id_ed25519-remote-ssh.pub    (public key)</p> <ol> <li> <p>Follow the 'quick start Using SSH Key' from the following tutorial: https://code.visualstudio.com/docs/remote/troubleshooting#_quick-start-using-ssh-keys BUT set PUBKEYPATH to id_ed25519-remote-ssh.pub</p> </li> <li> <p>Modify your <code>Users/USERNAME/.ssh/config</code> file locally</p> </li> </ol> <p>Add the following information to your config file: <pre><code>Host alma\nHostName alma.icr.ac.uk\nUser USERNAME\nIdentityFile ~/.ssh/id_ed25519-remote-ssh\n</code></pre></p> <ol> <li>Copy public key</li> </ol> <p>Open <code>/Users/USERNAME/.ssh/id_ed25519-remote-ssh.pub</code> and copy the key</p> <p><pre><code># command line\ncat /Users/USERNAME/.ssh/id_ed25519-remote-ssh.pub\n</code></pre> Copy the displayed key</p> <ol> <li> <p>Log into Alma normally</p> </li> <li> <p>Modify your <code>/home/USERNAME/.ssh/authorized_keys</code> on the server Use preferred text editor, here is an example using vi: <pre><code># command line\nvi (or any other text editor) /home/USERNAME/.ssh/authorized_keys\n</code></pre> Paste the key into the file</p> </li> </ol>"},{"location":"first_steps/#2-github-account","title":"2. Github account","text":"<p>If you don't have a github account, you can create one here.  Use your ICR email address for the GitHub account, and once you have one raise a ticket with the  scientific computing helpdesk and ask to be added to the ICR GitHub organisation.</p>"},{"location":"first_steps/#3-git-with-ssh-keys","title":"3. Git with ssh keys","text":"<p>Many applications are integrated with git and having it set up correctly from the outset will save problems down the line.  You want to have it set up with ssh access from Alma to GitHub.      - Check first if you have ssh keys set up.     - Otherwise generate a new one.     - Then add the ssh key to github.</p> <p>For gitlab, the ~/.ssh/config file needs the gitlab credentials added as: <pre><code># Private GitLab instance\nHost ralcraft.git.icr.ac.uk\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/id_ecdsa\n</code></pre> Note that is seems to prefer the ecdsa keygen: <code>ssh-keygen -t ecdsa -C comment</code> add the contents of the .pub file to the ssh keys on gitlab (search ssh-key).  </p>"},{"location":"first_steps/#4-the-alma-fileshare","title":"4. The Alma fileshare","text":"<p>Samba servers exist for mounting easily a remote system on your Machine, for both SCRATCH and RDS. This allows you to move files between your local machine and Alma and to edit files directly on Alma. If you prefer (or need to access home), there are various browser applications such as WinSCP for the file system.</p> <p>Note for MAC users: Press Command+K and enter the Samba server address or simply open the address in your browser to mount the remote system.  </p> <ul> <li>IOS To have quick access to the scratch and RDS, save scratch and RDS server links in you favourites. You can do so by going 'Finder'&gt;'Go'&gt;'Connect to server...' and typing in the one of the following server addresses:</li> <li>For scratch: smb://alma-fs</li> <li>For RDS: smb://rds.icr.ac.uk/DATA</li> </ul> <p>Save the server by pressing the '+' icon. Whenever you want to connect, repeat 'Finder'&gt;'Go'&gt;'Connect to server...' and choose the server you wish to connect to.</p> <ul> <li>Windows Using file explorer, right-click on 'This PC' and choose \"Add a network drive\". Enter these paths:  </li> <li>For scratch:  <code>\\\\alma-fs\\SCRATCH</code> </li> <li>For RDS: <code>\\\\rds.icr.ac.uk\\DATA</code> </li> </ul>"},{"location":"first_steps/#5-conda-and-mamba","title":"5. Conda and Mamba","text":"<p>You should initialise mamba and conda in your shell profile. This will make sure that the conda and mamba commands are available in your shell. Instructions are here: Conda and Mamba Initialise</p>"},{"location":"first_steps/#6-python-and-r-ondemand","title":"6. Python and R OnDemand","text":"<p>Make sure you can use the ondemand applications you will require here: Alma Open OnDemand. RStudio uses the Alma installation of R and does not allow for environments (unlike R in scripts which can be used in a mamba environment)  but jupyter notebooks can be used with a conda environment.  </p>"},{"location":"first_steps/#7-nextflow","title":"7. Nextflow","text":"<p>Alma has a shared installation of Nextflow, but you can also access nextflow through python virtual environments or conda environments. Instructions are here: Nextflow Nextflow is used to build analysis pipelines among others. A community effort was put to collect existing set of analysis pipelines and to save in 'nf-core' pipelines. nf-core pipelines are also available on Alma, the complexity is in the way the pipelines are run on the slurm executor. Instructions are here for getting started and running an nf-core pipeline.</p>"},{"location":"first_steps/#8-docker-and-singularity","title":"8. Docker and singularity","text":"<p>An alternative to conda environments is creating a docker image and running it on Alma through singularity.  Additionally, many bioinformatics tools come available as docker images that can be run on Alma through singularity. Instructions are here: Docker and Singularity</p> <p>For any help or questions email scientific computing.</p>"},{"location":"first_steps/#9-connection-to-alma-and-using-slurm","title":"9. Connection to Alma and using Slurm","text":"<p>Alma clusters are hosted on : alma.icr.ac.uk.</p> <p>You can access your /home/ repository via ssh, as follows: <pre><code>ssh alma.icr.ac.uk\n</code></pre> Alma is using Slurm queue system for running jobs through scheduling.  Users submit jobs, which are scheduled and allocated resources (CPU time, memory, etc.) by the resource manager.</p> <p>Multiple partitions exist on Alma clusters: interactive, compute, GPU, data-transfer, short, among others. Compute queue is the default one. Depending on the task to accomplish one partition could be more adapted than the others. For instance, use data-transfer partition to move files, use GPU partition to run GPU adapted software/code and use interactive or short partition for ephemeral tasks.</p> <p>It is important to note that any computation should not be made directly on the cluster head, but rather on a node. Below is an example of how to run an interactive session on a remote node, for 2h with 10GB max memory and 2 cores. <pre><code>srun --pty --mem=10GB -c2 -t 02:00:00 -p interactive bash\n</code></pre> A good practice is to set accurately the allocated resources (CPU time, memory and core numbers) and not to over-estimate them to get a chance to run fast your job. The node you connect to will depend on the resources you request, usually node01 if you require more compute power and node24 is less. </p> <p>For more info,  an extensive internal documentation is present on Nexus</p>"},{"location":"green/","title":"Sustainable computing","text":""},{"location":"green/#sustainable-computing-at-the-icr","title":"Sustainable Computing at the ICR","text":"<p>This is a collection of resources and advice on sustainable computing at the ICR. The advice comes from a collaboration between groups at the ICR engaged with the GreenDiSC initiative for sustainable computing.  </p> <ul> <li>Breast Cancer Research - Data Science Team, contact Santiago Madera.  </li> <li>Research Software Engineering Team, contact Rachel Alcraft.  </li> <li>Data Science Team, contact Jacob Househam.  </li> </ul> <p>For any help or questions email scientific computing and ask for the RSE team.  </p>"},{"location":"green/#a-quick-summary-of-practical-actions","title":"A quick summary of practical actions","text":""},{"location":"green/#what-can-you-do-to-be-more-sustainable","title":"What can you do to be more sustainable?","text":"<ol> <li>Measure your carbon footprint  The application developed by the RSE team green-alma is a web utility that measures the carbon cost of HPC jobs using the green-algorithms library. It is available on the ICR network with the password and username available on request from the RSE team - contact the schelpdesk to ask. You can use this to measure your jobs and carbon use over a given period of time or job id.  </li> <li>Write efficient code - The resources below will help you to write more efficient code and include internal training at the ICR. If your code is more efficient then it will cost less in terms of time, money and carbon. -- Efficient R programming -- Profiling R -- Top 10 tips to maximise Python Code Performance -- Profiling Python</li> <li>Test your code - If your code fails then it will need to be corrected and re-run which will cost. Running tests on your code will help to reduce the number of times you need to run it. Using continuous integration built into GitHub or GitLab can help with this. Do consider that the tests themselves cost, so an efficient balance is needed for how often the tests are run. -- testthat for R -- pytest for python Ask the RSE team for help on setting this up with continuous integration for your projects.  </li> <li>Reuse code - Using open source community tested libraries will mean that you are using code that has been tested and is efficient. Internally built shared and tested libraries will also be an efficient use of time and resources.  </li> <li>Check your job efficiency on the HPC - Your HPC jobs will use resources whether they are running optimally or idle. Check the efficiency of your jobs and make sure that they are running as efficiently as possible. Check efficiency of resources: <code>seff &lt;jobid&gt;</code> Check profile of a job: <code>scontrol show jobid -dd &lt;jobid&gt;</code> More info on job scheduling on nexus </li> <li>Developer time is expensive The time you spend optimising is a cost that should be considered. That time is also a carbon cost and a financial cost, and so you should consider the balance between the two. The cost of developer time is generally now greater than CPU time.  </li> <li>Your work is more important - the carbon footprint of your code is important, but, as a researcher working on generally small isolated projects where code efficiency has a minimal impact on the wider world, the work you are doing is more important.</li> <li>Storage is a big win - It may be that your biggest win in terms of carbon footprint is in storage, so consider how you can reduce the amount of data you are storing on an ongoing basis. Monitor and reduce continuously, and work with your team to embed this practice. The storage on RDS can be monitored on an interal dashboard:  RDS dashboard (VPN only) will show the data used and how it changes over time. Consider how you communicate with the test of your team and collaborators on data uses and storage as managing the data with an agreed plan will be less painful than trying to understand who needs what later down the line. In conversations with your colleagues and collaborators the message about the environmental impact of data storage can be highlighted.  </li> <li>Co-benefits - Remember that there are usually multiple advantages to anything, and these can help with your motivation. For example a more efficient algorithm will run faster and cost less financially as well as in carbon terms.  The green-alma app shows the financial cost as well as the carbon cost of your jobs.  </li> <li>Record what you are doing - keep a record of what you are doing and how you are reducing your carbon footprint. This will help you to see the impact of your changes and to communicate this to others.  </li> </ol>"},{"location":"green/#what-can-scientific-computing-do-to-be-more-sustainable","title":"What can Scientific Computing do to be more sustainable?","text":"<ol> <li>The efficiency of the data centres we use has a large impact on the carbon footprint of our work.  </li> <li>The ability to time-schedule jobs at periods of low energy use is important. Currently we do not have an automatic procedure for time scheduling jobs, nor do we have out of hours support to look after jobs if they were scheduled in this way. Here we need to balance the carbon footprint of our work with the practicalities of our work and this is in progress.  </li> <li>The ability to measure the carbon footprint of our work is important. We have created the green-alma app (VPN only) to monitor the carbon footprint of all jobs on Alma.  </li> </ol>"},{"location":"green/#training-and-resources","title":"Training and Resources","text":""},{"location":"green/#internal-training-at-the-icr","title":"Internal training at the ICR","text":"<p>These are courses run internally at the ICR, to sign up please visit the pages linked. If there are any courses without dates, please sign on to the waiting list, and also feel free to contact the RSE team for more information.  </p> <ul> <li>IEMA Sustainability at the ICR Audience: All staff. As a world-leading research centre, promoting health is at the heart of what we do. We embrace opportunities to advance social, economic and environmental health in our activities and make the ICR a more sustainable organisation. Every member of staff at the ICR plays a part in helping us to achieve these goals. We\u2019ve taken some major steps towards this already, like developing science-based targets to bring down our carbon emissions and using the UN Sustainable Development Goals as a framework for how we holistically approach sustainability. We\u2019re inviting all members of staff to participate in these sustainability training courses. They\u2019ll provide opportunities to develop actions and initiatives to create change in your area of work, and to learn what the ICR is doing towards sustainability. These courses lead to a professional certificate from IEMA, (the Institute of Environmental Management and Assessment). </li> <li>Software carpentries  At the ICR, the RSE team, in collaboration with the Data Science team, deliver carpentry courses in Python, R, Git and Unix. These are designed to help you to be more efficient in your coding and to work more effectively with others, and by using best practices in coding you can reduce the carbon footprint of your work. Where we can, we use the carpentry materials from the Software Carpentry organisation.   -- Python   -- R   -- Bash   -- Git   -- Image processing with python   -- Genomics   -- Python optimisation - to be announced soon  </li> <li>Induction on computational sustainability - this is to be announced soon</li> </ul>"},{"location":"green/#resources","title":"Resources","text":"<ul> <li>Green Software for practitioners by the Linux Foundation, a comprehensive online training course with certifcate on succesful completetion. </li> <li>The Turing Way a guide to sustainable software development practices.  </li> <li>Reproducible research with python from Code refinery. </li> <li>Some slurm docs - internal nexus doc  and additional FAQs </li> <li>Recycling of our equipment by the StoneGroup </li> <li>The Slough Data Centre </li> </ul>"},{"location":"green/#an-overview-of-sustainable-practices-in-computing","title":"An overview of sustainable practices in computing","text":"<p>Much of the advice comes from the book <code>Building Green Software</code> by Anne Currie, Sarah Hsu and Sara Bergman (O'Reilly ).  </p>"},{"location":"green/#overview-of-goals","title":"Overview of goals","text":"<p>The lifecycle of a software product includes the energy used in the production of the hardware, the energy used in the software development, the energy used in the operation of the software, and the energy used in the disposal of the hardware. The goal of green software is to reduce the carbon footprint of software.  Greenhouse gases (GHGs) are primarily carbon dioxide (CO2), methane, nitrous oxide, and fluorinated gases, with CO2 being the most problematic human emmission. To measure them in a single comparable unit we use carbon dioxide equivalent (CO2e), which we refer to as 'carbon'.</p>"},{"location":"green/#measuring-carbon","title":"Measuring carbon","text":"<p>The carbon footprint is at all points in the lifecycle of a software project, but here we are talking about what you can do. The disposal of hardware, the purchasing and maintanenace and upgarde of the servers, the choice of data centres the energy used in those data centres are a central decision. Engagement in these decisions is welcomed and encouraged, but here we are talking about the carbon footprint of the code you write and the jobs you run.  </p> <p>To measure these you can keep an inventory of what you use, measure the carbon cost of your Alma jobs using green-alma and monitor your storage.  </p> <p>Remember that this is a small piece of the puzzle, and everything you do contributes to the overall carbon footprint of the ICR and of your own activities. How you choose to travel to work is just as important as how you write your code. </p>"},{"location":"green/#code-efficiency","title":"Code efficiency","text":"<p>Developer time is a very expensive resource, and the efficiency of code has to be balanced against that time; the readibility of code for others; and the maintainability of the code. A pragmatic approach is recommended where awareness of the development quality is balanced against the competing demands of time and output. Reuse, testing, design and profiling are all important in this process. Please attend training courses at the ICR, read the resources and ask the RSE team for help. Use version control systems and don't be afraid that anyone is inspecting your code, this is a collective effort to try to reduce the carbon footprint of our work not a judgement on your coding skills.  </p>"},{"location":"green/#being-carbon-aware","title":"Being carbon aware","text":"<p>Some of the big cloud providers are leading the way on carbon awareness, with Microsoft having implemented a carbon-aware Windows Update process. Carbon awareness is the understanding that not all energy is equivalent, and the time of day and the source of the energy are important. If you can move your jobs around data centres in different locations and at different times you have an opportunity to choose to use energy from renewable sources and at times when the energy is less carbon intensive. These different techniques are called demand shifting, and specfically time shifting and location shifting. </p> <p>However, we cannot control all these pieces as ICR users. We can control the jobs we choose to run, the efficiency of that code, the efficiency of the resource use, how well we monitor it and how we clean up the logs. We can put pressure on the ICR to adopt carbon-aware practices and to make the data centres more efficient. We can also choose to use the cloud providers that are more carbon aware. </p>"},{"location":"green/#operational-efficiency","title":"Operational efficiency","text":"<p>Rightsizing is a buzzword, it means not overprovisioning resources and is one of the cheapest green actions that we can take. In Alma terms it can mean we don't ask for more resources than we need. Cluster scheduling, as done on slurm, means that jobs can be sumbitted and run when there is space - this builds an element of right-sizing in to our standard practices. Operational efficiency is improved by not having always-on servers but by using resources provisioned on demand. We do not use cloud services at the moment, but this could be a future way to improve operational efficiency by expanding and contracting resources as needed.</p>"},{"location":"green/#hardware-efficiency","title":"Hardware efficiency","text":"<p>The HPC cluser is managed and upgraded on a schedule based on standard practices. The carbon cost of production and disposal need to be factored into that relatively short time period of equipment use. Extending the life of hardware is one of the best ways to reduce the carbon footprint, there is a balance to be struck between the cost of maintenance and the cost of disposal and production. </p>"},{"location":"green/#machine-learning-and-ai","title":"Machine learning and AI","text":"<p>Improvements in hardware have been an enabler for AI and machine learning, but the carbon cost of these models is high. The size and use of models is growing, and they carry ethical implications. Generally ML models are not time critical so they can benefit from demand shifting. Also, there are many pre-trained models available so using an existing model is more efficient than training a new one. If there is not a model for your needs, transfer learning will take an existing model and adapt it to your data.</p>"},{"location":"green/#measuring-and-monitoring","title":"Measuring and monitoring","text":"<p>When talking about the carbon costs we often talk about scopes 1-3, where scope 1 is the direct emissions from the activities of the organisation, scope 2 is the indirect emissions from the energy used by the organisation, and scope 3 is the indirect emissions from the supply chain and the use of the products. </p> <p>There is little clarity for what this means for software creation. But, from your persepctive as a user of the HPC facilities, your main responsibity is for the costs of your compute time and storage on Alma and RDS. These can be measured and monitored with green-alma and sjane respectively. You can take action to monitor these to understand the appropriateness of the costs. If you have very high Alma use it does not mean you are using an inappropriate amount of resources, but it is important to be aware of the costs and to be able to justify them. </p>"},{"location":"green/#co-benefits","title":"Co-benefits","text":"<p>Eco-friendly practices have co-benefits that can help motivate you, and also sell the idea to colleagues and management.  </p> <ul> <li>Money saving - the cost of energy is a significant part of the cost of running a data centre, and so saving energy will save money.  </li> <li>Time saving - if the code is more efficient it will run faster and so save time.  </li> <li>Better code - the code will be more efficient and better tested so more reliable, easier to maintain and understand.  </li> <li>Better collaboration - the code will be more readable and so easier to work with others.  </li> <li>Better science - clearer code will be more reliable, reusable and reproducible.  </li> </ul>"},{"location":"conda/basics/","title":"Conda environment management","text":"<p>A central theme in conda is that of environments. An environment is an object that we create, which enables us to  \"contain\" programming languages and any packages that we want to install to use alongside it, in a single place.</p> <p>Environments can be created, updated and removed.  We can activate and deactivate different environments to use with different projects.</p>"},{"location":"conda/basics/#1-creating-a-new-environment","title":"1. Creating a new environment","text":"<p>It is best practice, when we create a new environment, to define the version of our programming language we want  to use. This is much easier to setup now than change later. </p> <p>To create a new environment, run the command</p> <ul> <li>Python</li> </ul> <pre><code>mamba create --name &lt;env name&gt; python=&lt;version&gt;\n</code></pre> <ul> <li>R</li> </ul> <pre><code>mamba create --name &lt;env name&gt; r-base=&lt;version&gt;\n</code></pre> <p>This will create a new environment that has an associated name that you have provided, here represented by <code>&lt;env name&gt;</code>, and a corresponding version, here represented by <code>&lt;version&gt;</code>. As a concrete example, we can create an environment called \"tmp_python\" using Python 3.11:</p> <pre><code>mamba create --name tmp_python python=3.11\n</code></pre> <p>When prompted <pre><code>Confirm changes: [Y/n]\n</code></pre> write Y and press enter. </p> <p>The process will probably take a minute or longer to complete.</p>"},{"location":"conda/basics/#2-installing-packages-into-an-active-environment","title":"2. Installing packages into an active environment","text":"<p>After creating an environment, it can be activated via</p> <pre><code>mamba activate &lt;env name&gt;\n</code></pre> <p>The lower-left hand corner of your terminal will probably then look like: <code>(&lt;env name&gt;) [&lt;username&gt;@login(alma) ~]$</code>.</p> <p>To install a package we should run the command</p> <pre><code>mamba install &lt;package name&gt;\n</code></pre> <p>where <code>&lt;package name&gt;</code> is the name of the package we want to install, for example, <code>numpy</code>.</p> <p>Note to R users: All R package names should be preceded by <code>r-</code>. For example, <code>dplyr</code> should be <code>r-dplyr</code>. </p> <p>If you require a specific package version, you can instead use the syntax <code>&lt;package name&gt;=&lt;version&gt;</code>, for example, <code>numpy=1.23.5</code>.</p> <p>You can now start a console session (by writing <code>python</code> or <code>R</code> and hitting enter), and check that your package is indeed available.</p>"},{"location":"conda/basics/#3-deactivating-and-removing-environments","title":"3. Deactivating and removing environments","text":"<p>We can deactivate an active environment by pressing ctrl+d. </p> <p>If you wish to remove an environment that you no longer need, first ensure that it has been deactivated, then run the command</p> <pre><code>mamba env remove &lt;env name&gt;\n</code></pre>"},{"location":"conda/mamba-first/","title":"Using Mamba (conda) for the first time","text":"<p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\n</code></pre></p> <p>In order to use <code>conda</code>, users should add the EasyBuild modules to their user path and load the <code>Mamba</code> module:</p> <pre><code>module use /opt/software/easybuild/modules/all\nmodule load Mamba\n</code></pre> <p>You can check that the module has been loaded successfully by running <code>module list</code>, which should return the output:</p> <pre><code>Currently Loaded Modules:\n  1) slurm-21.08.5   2) Mamba/23.1.0-0\n</code></pre> <p>In addition to any other modules you may have already loaded. You can further check that the <code>mamba</code> and <code>conda</code> commands are now available for you to use, via <code>mamba --version</code>, which should return the output:</p> <pre><code>mamba 1.4.0\nconda 23.1.0\n</code></pre>"},{"location":"conda/mamba-first/#2-initialize-your-shell","title":"2. Initialize your shell","text":"<p>After the module has been loaded, you should now initialize your shell. The easiest way to achieve this, is to run</p> <pre><code>mamba init\n</code></pre> <p>This should generate the output:</p> <pre><code>                  __    __    __    __\n                 /  \\  /  \\  /  \\  /  \\\n                /    \\/    \\/    \\/    \\\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588/  /\u2588\u2588/  /\u2588\u2588/  /\u2588\u2588/  /\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n              /  / \\   / \\   / \\   / \\  \\____\n             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n            / _/                       \\_____/  `\n            |/\n        \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557\n        \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n        \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n        \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n        \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\n        \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\n\n        mamba (1.4.0) supported by @QuantStack\n\n        GitHub:  https://github.com/mamba-org/mamba\n        Twitter: https://twitter.com/QuantStack\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/condabin/conda\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/bin/conda\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/bin/conda-env\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/bin/activate\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/bin/deactivate\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/etc/profile.d/conda.sh\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/etc/fish/conf.d/conda.fish\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/shell/condabin/Conda.psm1\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/shell/condabin/conda-hook.ps1\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/lib/python3.10/site-packages/xontrib/conda.xsh\nno change     /opt/software/easybuild/software/Mamba/23.1.0-0/etc/profile.d/conda.csh\nmodified      /home/&lt;username&gt;/.bashrc\n\n==&gt; For changes to take effect, close and re-open your current shell. &lt;==\n\nAdded mamba to /home/&lt;username&gt;/.bashrc\n\n==&gt; For changes to take effect, close and re-open your current shell. &lt;==\n</code></pre> <p>It is important that we pay attention to the last part of this output: Close your Alma terminal and open it up again. You can litteraly close and open your Alma terminal, or alternatively run:</p> <pre><code>$source ~/.bashrc\n</code></pre>"},{"location":"conda/mamba-first/#3-using-conda-post-initialization","title":"3. Using conda post-initialization","text":"<p>After you have started a fresh terminal session, you do not need to follow the previous steps again. Check that you can run</p> <pre><code>mamba --version\n</code></pre> <p>which as before, should now return:</p> <pre><code>mamba 1.4.0\nconda 23.1.0\n</code></pre> <p>You should now be ready to use conda.</p>"},{"location":"conda/mamba-first/#4-adding-preloaded-path-for-alma","title":"4. Adding preloaded path for Alma","text":"<p>Alma has some preloaded environments that you can use if you are looking to just use a single tool (you can't add to the environments). You can add these to your path by running:</p> <pre><code>conda config --append envs_dirs /opt/software/applications/anaconda/3/envs/\n</code></pre> <p>You can then see that you have these environments by running:</p> <pre><code>conda info --envs\n</code></pre> <p>And activate them by running, for example:</p> <pre><code>conda activate star2.7.6a\n</code></pre>"},{"location":"conda/mamba-venvs/","title":"Creating Mamba Environments","text":"<p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\n</code></pre></p> <p>This assumes that you have already followed the using mamba for the first time tutorial on this site. Conda environments can be shared amongst users, and for yourself you can share them between different applications, e.g., all your nextflow pipelines can use the same environment.</p> <p>Mamba environments manage dependencies and packages for you, and are the preferred way to install R packages.  Most R packages can be installed from mamba wiby prepending \"r-\" eg \"r-tidyverse\" etc.  Only where the package does not exist is it recommended to use package.install in R.</p> <p>Using conda environments enables you to use more up-to-date versions of software than are available in the standard modules.  </p> <p>It also facilitates sharing, re-use, and reproducibility of your work.</p> <p>N.b. after creating a new environment, you can activate it with the command: <pre><code>mamba activate myenv\n</code></pre> To deactivate it use: <pre><code>mamba deactivate\n</code></pre></p>"},{"location":"conda/mamba-venvs/#creating-environments","title":"Creating environments","text":""},{"location":"conda/mamba-venvs/#1-in-the-standard-way","title":"1. In the standard way","text":"<p><pre><code>mamba create --name myenv  \n</code></pre> This will be located in the conda home directory set up in your path.  To find the directories used for environments call: <pre><code>conda config --show envs_dirs\n</code></pre> You will see any shared (read only) environments you have access too along with your own personal environments. Note: because the default is your home directory which is does not have much space, you may want to move the default to SCRATCH.  </p> <p>Once activated anything you install will be installed in the active environment, including any python pip installs.  To install packages into the environment, e.g. numpy: <pre><code>mamba install numpy\n</code></pre></p>"},{"location":"conda/mamba-venvs/#2-in-a-specific-location","title":"2. In a specific location","text":"<pre><code>mamba create --prefix /path/to/env myenv\n</code></pre>"},{"location":"conda/mamba-venvs/#3-with-a-specific-version-of-python","title":"3. With a specific version of Python","text":"<pre><code>mamba create --name my-env python=3.8\n</code></pre>"},{"location":"conda/mamba-venvs/#4-with-a-specific-version-of-a-package","title":"4. With a specific version of a package","text":"<pre><code>mamba create --name my-env package=1.0\n</code></pre>"},{"location":"conda/mamba-venvs/#5-with-a-specific-version-of-a-package-and-python","title":"5.  With a specific version of a package and Python","text":"<pre><code>mamba create --name my-env python=3.8 package=1.0\n</code></pre>"},{"location":"conda/mamba-venvs/#6-with-a-specific-version-of-r","title":"6. With a specific version of R","text":"<pre><code>mamba create --name my-env /path/to/env r-base=4.1\n</code></pre>"},{"location":"conda/mamba-venvs/#some-common-environments-using-channels","title":"Some common environments using channels","text":"<p>Mamba uses channels to search for packages. A package could be present in multiple channels.  A channel is specified using the -c argument. A channel is an independent and isolated repo structure that is used to classify and administrate more easily a package server. It comes with a repodata.json file that is the index of all available packages.</p> <p>Examples of existing channels: 1. bioconda 2. conda-forge 3. defaults </p>"},{"location":"conda/mamba-venvs/#1-from-the-bioconda-channel-samtools-and-bcftools-packages","title":"1. From the bioconda channel: samtools and bcftools packages","text":"<pre><code>mamba create --name my-env -c bioconda samtools bcftools r-base=4.3\n</code></pre>"},{"location":"conda/mamba-venvs/#2-from-the-bioconda-channel-nextflow-and-nf-core-tools","title":"2. From the bioconda channel: nextflow and nf-core tools","text":"<pre><code>mamba create --name my-env -c bioconda nextflow nf-core\n</code></pre>"},{"location":"conda/mamba-venvs/#3-from-the-conda-forge-channel-r-with-tidyverse","title":"3. From the conda-forge channel: R with tidyverse","text":"<pre><code>mamba create --name my-env -c conda-forge r-tidyverse\n</code></pre>"},{"location":"conda/mamba-venvs/#4-from-the-conda-forge-channel-python-with-pandas-and-numpy","title":"4. From the conda-forge channel: Python with pandas and numpy","text":"<pre><code>mamba create --name my-env -c conda-forge python=3.8 pandas numpy\n</code></pre>"},{"location":"conda/mamba-venvs/#sharing-environments","title":"Sharing environments","text":"<p>You can save your environment to a file and share it with others. <pre><code>mamba env export --name my-env &gt; my-env.yml\n</code></pre> This will create a file called my-env.yml in your current directory. You can load this environment on another machine with the command: <pre><code>mamba env create --file my-env.yml\n</code></pre></p>"},{"location":"conda/mamba-venvs/#removing-environments","title":"Removing environments","text":"<p>To remove an environment, use the command: <pre><code>mamba env remove --name my-env\n</code></pre> This will remove the environment from your system.  </p>"},{"location":"conda/mamba-venvs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"conda/mamba-venvs/#1-condamamba-environment-is-broken","title":"1.  conda/mamba environment is broken","text":"<p>If you have a broken conda/mamba environment, you can try to fix it by removing the environment and creating it again. You can do this with the commands (conda or mamba): <pre><code>conda env remove --name myenv\nconda create --name myenv\n</code></pre></p> <p>However sometimes the environments are so badly broken you cannot use any conda commands at all and get unhandled or unknown exceptions.  In these rare circumstances you can totally refresh your installation by:  - Renaming the .conda directory in your home directory  - Renaming the .condarc file in your home directory  - Removing the conda init section in your .bashrc file which means deleting this entire section <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\nDelete all this stuff too, as well as the &gt;&gt;&gt; conda initialize &gt;&gt;&gt; line\nand the &lt;&lt;&lt; conda initialize &lt;&lt;&lt; line\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre>  - Go through the init process to set up conda and your environment again.</p>"},{"location":"conda/mamba-venvs/#2-there-is-no-space-left","title":"2. There is no space left!","text":"<p>If you are running out of space in your home directory, you can move the conda environment to SCRATCH.  You can do this by creating a simlink to the SCRATCH directory.  - Copy your home/.conda directory to SCRATCH somewhere you will remember it is!  - Remove the .conda directory in your home directory   - Create a simlink to the SCRATCH directory <pre><code>cp -r ~/.conda /data/scratch/DCO/DIGOPS/SCIENCOM/ralcraft/.conda_simlink\nrm -r -f ~/.conda\nln -s /data/scratch/DCO/DIGOPS/SCIENCOM/ralcraft/.conda_simlink ~/.conda\n</code></pre></p>"},{"location":"conda/mamba-venvs/#further-reading","title":"Further reading","text":"<ul> <li>Mamba documentation</li> </ul>"},{"location":"conda/python-ondemand/","title":"Using Jupyter OnDemand on Alma - Environments","text":"<p>Creating an environment with your installations will enable you to use Jupyter notebooks on Alma in a reproducible manner. The environment can be shared with colleagues, help desk, and collaborators to ensure that the same environment is used for the same results. It also means that in the future you can recreate the environment to run the same notebooks.</p> <p>You will first need to create the environment on the command line and activate it as a kernel for Jupyter.</p> <ol> <li> <p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\nmamba --version\n</code></pre></p> </li> <li> <p>Follow the steps for creating a new environment in the mamba documentation and activate it.</p> </li> <li> <p>Install the ipykernel package <pre><code>mamba install -c conda-forge ipykernel\n</code></pre></p> </li> <li> <p>Activate the kernel with python <pre><code>python -m ipykernel install --user --name=my-env-name\n</code></pre></p> </li> <li> <p>Start Jupyter OnDemand</p> </li> </ol> <p>The dropdown for the kernels will now include this environment: </p>"},{"location":"conda/python-scripts/","title":"Using python scripts on Alma - Environments","text":"<p>Creating an environment with your installations will enable you to use python scripts on Alma in a reproducible manner. The environment can be shared with colleagues, help desk, and collaborators to ensure that the same environment is used for the same results. It also means that in the future you can recreate the environment to run the same scripts.</p> <ol> <li> <p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\n</code></pre></p> </li> <li> <p>Check that mamba is working <pre><code>mamba --version\n# which should return:\nmamba 1.4.0\nconda 23.1.0\n</code></pre> If not make sure you have followed the setup mamba instructions</p> </li> <li> <p>Create a new environment <pre><code>mamba create --name my-env-name python=3.10\nmamba activate my-env-name\n# locally as an alternative\nmamba create --prefix ./my-env-name python=3.10\nmamba activate ./my-env-name\n</code></pre></p> </li> <li> <p>Check you have the environment you expect <pre><code>python --version\n</code></pre></p> </li> <li> <p>Installing packages It is preferable to use mamba to install python packages like matplotlib etc <pre><code>mamba install matplotlib\n</code></pre></p> </li> <li> <p>If you need to use a python script, you can use the python command <pre><code>pip install matplotlib\n</code></pre> But it is better to use mamba to install packages where possible.</p> </li> <li> <p>Export and import the environment <pre><code># exporting the environment\nmamba env export --name my-env-name &gt; my-env-name.yml\npip freeze &gt; my-env-name.txt\n\n# Loading the environment\nmamba env create --file my-env-name.yml\npip install -r my-env-name.txt\n</code></pre></p> </li> <li> <p>Deactivate the environment <pre><code>mamba deactivate\n</code></pre></p> </li> <li> <p>Remove the environment Warning this means to remove all the packages and the environment, only do this if you really want to. <pre><code>mamba remove --name my-env-name --all\n</code></pre></p> </li> <li> <p>Check what environments I have available <pre><code>mamba env list\n</code></pre></p> </li> </ol>"},{"location":"conda/python-venv/","title":"Using python scripts on Alma - python venv","text":"<p>Creating an environment with your installations will enable you to use python scripts on Alma in a reproducible manner. The environment can be shared with colleagues, help desk, and collaborators to ensure that the same environment is used for the same results. It also means that in the future you can recreate the environment to run the same scripts.</p> <p>An alternative to conda/mamba is the python virtual environment. This is lighter and simpler than mamba, but not as powerful - it does not have the ability to also install some bioinformatics tools available on conda-forge but is limited to python libraries on pypi. Nevertheless, it is a good option for simple python scripts.</p> <ol> <li> <p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\n</code></pre></p> </li> <li> <p>Check the version of python <pre><code>python --version\n</code></pre></p> </li> <li> <p>Create a new environment <pre><code>python -m venv my-env-name\nsource my-env-name/bin/activate\n</code></pre></p> </li> <li> <p>Installing packages You can pip install the packages, optionally with version (=1.3.2). <pre><code>pip install matplotlib\n</code></pre></p> </li> <li> <p>Installing packages from a requirements file The packages can be listed in a simple text file - a requirements file. This can be installed with pip. This is an approximation of a reproducible environment but may not cover versions and dependencies. <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Deactivate the environment <pre><code>deactivate\n</code></pre></p> </li> <li> <p>Remove the environment This is installed in the directory, so can be removed with <code>rm -rf my-env-name</code>. </p> </li> </ol>"},{"location":"conda/r-github/","title":"Installing R Packages from GitHub","text":"<p>R packages can be installed from GitHub, and these can be troublesome in terms of their dependencies as they are less community tested. This example shows installation of the TwoSampleMR package from GitHub, with an explanation of why some packages are manually installed.  </p> <p>Note if you are trying to install a package and you get stream of error messages, the first  thing to try it to install those packages in mamba with \"r-\" prepended as below in step 3.</p> <p>If you haven't already, follow the setup mamba instructions to get mamba working.</p> <ol> <li> <p>Log on to an interactive alma session and move somewhere sensible on scratch <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty --mem=10GB -c 1 -t 30:00:00 -p interactive bash\ncd /data/scratch/somewhere/sensible\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment <pre><code>mamba create --name mamba-TwoSampleMR -c bioconda r-tidyverse r-remotes r-base=4.3\nmamba activate mamba-TwoSampleMR\n</code></pre></p> </li> <li> <p>Manually install some packages in a dependency safe way <pre><code>mamba install r-meta\nmamba install r-nloptr\nmamba install r-lme4\n</code></pre></p> </li> <li> <p>Install custom package <pre><code>R -e 'remotes::install_github(\"MRCIEU/TwoSampleMR\")'\n</code></pre></p> </li> </ol>"},{"location":"conda/r-ondemand/","title":"RStudio OnDemand - Environments","text":"<p>Uses the central build and not environments</p> <p>It is possible to use Jupyter notebook with R on Alma for something interactive.  </p> <p>To do this you create a conda environment in similarly to python, and then install R and the IRkernel.  This is a bit more complicated than python, but it is possible.  You can then run R in a Jupyter notebook.</p> <ol> <li> <p>Use conda to create a new environment in the mamba R documentation and activate it. <pre><code>conda create --name my-conda-r -c conda-forge -c bioconda -c r -c defaults r-base=4.3\nconda activate my-conda-r\n</code></pre></p> </li> <li> <p>Install the r-irkernel package <pre><code>conda install ipykernel r-irkernel\n</code></pre></p> </li> <li> <p>Activate the kernel with python <pre><code>python -m ipykernel install --user --name=my-conda-r\n</code></pre></p> </li> <li> <p>Start Jupyter OnDemand</p> </li> </ol> <p>The dropdown for the kernels will now include this environment: </p>"},{"location":"conda/r-scripts/","title":"Using R Scripts on Alma - Environments","text":"<p>Creating an environment with your installations will enable you to use R on Alma in a reproducible manner. The environment can be shared with colleagues, help desk, and collaborators to ensure that the same environment is used for the same results. It also means that in the future you can recreate the environment to run the same notebooks.</p> <p>If you haven't already, follow the setup mamba instructions to get mamba working.</p> <ol> <li> <p>First get an interactive session: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\nmamba --version\n</code></pre></p> </li> <li> <p>Follow the steps for creating a new environment in the mamba documentation and activate it. Create a mamba environment, just like with python, though there are a few modules we know we are likely to want with R.  <pre><code>mamba create --name my-mamba-r -c conda-forge -c bioconda -c r -c defaults r-base=4.3\nmamba activate my-mamba-r\n</code></pre></p> </li> <li> <p>Check you have the environment you expect <pre><code>R --version\n</code></pre></p> </li> <li> <p>Load CMake You will need to load the CMake module to install some R packages that need to be installed from source. <pre><code>module load CMake\n</code></pre></p> </li> <li> <p>Installing packages It is preferable to use mamba to install R packages like ggplot2 etc. In general, if there is a package that you could install with install(packages), you can install it with mamba install with an \"r-\" in the front - not always, but it is the best way to try. <pre><code>mamba install r-ggplot2\n</code></pre></p> </li> <li> <p>Packages that do not have an \"r-\" module Can be loaded in an R script in the usual way, or form the command line like: <pre><code>R -e \"install.packages('ggplot2', repos='https://cloud.r-project.org')\"\n</code></pre></p> </li> <li> <p>Export and import the environment <pre><code># exporting the environment\nmamba env export --name my-mamba-r &gt; my-mamba-r.yml\n</code></pre></p> </li> <li> <p>Deactivate the environment <pre><code>mamba deactivate\n</code></pre></p> </li> <li> <p>import the environment <pre><code>mamba env create --file my-mamba-r.yml\n</code></pre></p> </li> </ol>"},{"location":"faqs/faqs/","title":"Frequently Asked Questions Using Alma","text":""},{"location":"faqs/faqs/#1-how-do-i-get-access-to-alma","title":"1. How do I get access to Alma?","text":"<p>Follow the Getting Started tutorial on nexus. Also follow the First Steps tutorial on this site.</p>"},{"location":"faqs/faqs/#2-who-do-i-ask-for-help","title":"2. Who do I ask for help?","text":"<p>Send an email to Scientific Computing HelpDesk </p>"},{"location":"faqs/faqs/#3-disk-space-problems","title":"3. Disk Space Problems","text":"<p>The quota is 10GB. If you are running out of space, you can check your disk usage with the following commands: <pre><code>du -sh .conda/\ndu -sh /home/ralcraft/\nls -al\n</code></pre></p>"},{"location":"faqs/faqs/#4-error-could-not-install-packages-due-to-an-oserror-errno-122-disk-quota-exceeded-pathtopython310","title":"4. ERROR: Could not install packages due to an OSError: [Errno 122] Disk quota exceeded: '/path/to/python3.10'","text":"<p>Does conda need to be cleaned out? Check for conda cache files: <pre><code>du -sh .conda/\nconda clean --all\n</code></pre></p>"},{"location":"faqs/faqs/#5-my-singularity-image-did-not-pull","title":"5. My singularity image did not pull!!!","text":"<p>Occasionally large singularity images can cause network issues. Try again, or try with wget. If this failed: <pre><code>singularity pull  --name depot.galaxyproject.org-singularity-mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb-7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0.img.pulling.1715165870091 https://depot.galaxyproject.org/singularity/mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb:7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0 &gt; /dev/null\n</code></pre> Then the image you were trying to pull is the second parameter in the command, so https:// etc. The name that you want this image to have is the first parameter in the command up to where it says pulling, so: - image = https://depot.galaxyproject.org/singularity/mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb:7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0 - name = depot.galaxyproject.org-singularity-mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb-7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0.img</p> <p>Replicate the command with wget like: <pre><code>wget https://depot.galaxyproject.org/singularity/mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb:7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0 -O depot.galaxyproject.org-singularity-mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb-7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0.img\n</code></pre> It should go into your singularity or APPTAINER directory, which you should have set up in your home directory. To check it do: <pre><code>echo $APPTAINER_CACHEDIR\necho $SINGULARITY_CACHEDIR\necho $NXF_SINGULARITY_CACHEDIR\n</code></pre></p> <p>Where APPTAINER_CACHEDIR is the new place singularity goes, and SINGULARITY_CACHEDIR is the old place, and NXF_SINGULARITY_CACHEDIR is the place Nextflow looks for singularity images.</p> <p>To change them do (for whichever one you want to change): <pre><code>export APPTAINER_CACHEDIR=/data/scratch/your/path/username/.singularity/cache\n</code></pre></p>"},{"location":"faqs/faqs/#6-my-condamamba-environment-is-broken","title":"6. My conda/mamba environment is broken","text":"<p>If you have a broken conda/mamba environment, you can try to fix it by removing the environment and creating it again. You can do this with the commands (conda or mamba): <pre><code>conda env remove --name myenv\nconda create --name myenv\n</code></pre></p> <p>However sometimes the environments are so badly broken you cannot use any conda commands at all and get unhandled or unknown exceptions.  In these rare circumstances you can totally refresh your installation by: 1. Renaming the .conda directory in your home directory 2. Renaming the .condarc file in your home directory 3. Removing the conda init section in your .bashrc file which means deleting this entire section <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\nDelete all this stuff too, as well as the &gt;&gt;&gt; conda initialize &gt;&gt;&gt; line\nand the &lt;&lt;&lt; conda initialize &lt;&lt;&lt; line\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> 4. Go through the init process to set up conda and your environment again.  </p>"},{"location":"ides/campus/","title":"VSCode extra features","text":"<p>For access to the extra features in VSCode such as co-pilot, you need to sign up to the educators program.  </p> <p>There is a bit of a 2-stage step for the VSCode license, you need to be a member of the ICR GitHub organisation,  but then separately you need to apply to the GitHub campus program as a teacher/researcher.  </p> <ol> <li>You need a GitHub account with your icr email address, request to be added from schelpdesk. You don\u2019t have to be in the GitHub organisation, you just need the academic email address, but it makes sense to associate yourself to the ICR GitHub as there are some other free things (like private GitHub actions).  </li> <li>Additionally you also need to apply to the GitHub campus programme as an individual researcher in an academic institution. Researcher@GitHub campus. You need to sign up to the github campus programme as a teacher, which is also a researcher, using that GitHub account linked to the icr email address. You need to provide proof that you work for a university \u2013 a photo of your lanyard might do it, or you can ask HR to give you a letter saying you work for the ICR.  </li> </ol> <p>It takes a couple of days for the application to go through, and then you will have access to those benefits.  E.g. in VSCode you download the copilot extension and sign in with github and you will have access to it (it has been a while since I did that so give me a shout if that bit is not obvious!).</p> <p>Any problems ask for help from schelpdesk!</p>"},{"location":"ides/jupyter/","title":"Installing Jupyter Lab","text":""},{"location":"ides/jupyter/#on-windows","title":"On Windows","text":"<p>At the ICR, the Company Portal has a Jupyter Lab app for Windows.  </p>"},{"location":"ides/jupyter/#on-a-mac","title":"On a mac","text":"<p>Follow this link for the mac installation instructions:  Anaconda mac install</p>"},{"location":"ides/jupyter/#setting-up","title":"Setting up","text":"<p>Once installed the Anaconda Suite will appear on the Start Menu. Search for Anaconda Prompt </p> <p>Navigate to a folder that you want to save files in, eg c:/Users/ralcraft/Documents, and then type the following command: <pre><code>jupyter lab\n</code></pre> This will start Jupyter Lab in your default web browser.</p>"},{"location":"ides/jupyter/#check-it-works","title":"Check it works","text":"<p>You should see the default option of a Python 2 kernal, click it. </p> <p>Then a jupyter notebook will appear, you can test it is working by simply typing in  <pre><code>print(\"Hello World\")\n</code></pre> Press the play arrow at the top - if you see the output <code>Hello World</code> then it is working correctly. </p> <p>Any problems, contact schelpdesk asking to be put in touch with the RSE team.</p>"},{"location":"ides/remote/","title":"Remote Debugging with VSCode","text":"<p>We have recently made it possible to connect to Alma from VSCode - though only to the login nodes.</p> <p>Instructions are fairly brief covering all the components involved - any more information needed about any component contact the helpdesk.</p> <p>The full details are given by microsoft here: remote-ssh with vscode </p> <p>An adaptation for Alma follows:  </p>"},{"location":"ides/remote/#installation","title":"Installation","text":"<ul> <li>You need to have VSCode installed</li> <li>You need to download the remote-ssh extension from the Visual Studio Marketplace.</li> </ul>"},{"location":"ides/remote/#ssh-host-setup","title":"SSH host setup","text":"<ul> <li>You need an ssh key/pair for Alma set up. You can find the instructions in the 'First Steps' tab under 'Setting up ssh key for Alma'.</li> </ul> <p>EITHER: </p>"},{"location":"ides/remote/#connect-to-a-remote-host-login-node","title":"Connect to a remote host: login node","text":"<ul> <li>In VSCode you need to check that you can ssh onto alma in a VSCode terminal.</li> <li>In VSCode select \"Connect to host\" after clicking on the \"remote window\" icon in the far left bottom of the toolbar</li> <li>enter <code>ssh username@alma.icr.ac.uk</code></li> <li>Select the directory with your ssh key</li> </ul> <p>This will now create the connection. In the future you will be able to select this from the list when you choose a remote connection.</p> <p>Alma has a 200MB memory limit for the login nodes, so you will need to use the interactive nodes for any real work. Do not use login node to run code, use compute node. Once connected to your code with the remote-ssh extension you can use edit your code with the login node but can have the terminal in VSCode as part of an interactive session for running it.</p>"},{"location":"ides/remote/#interactive-session-in-vscode-terminal","title":"Interactive session in VSCode terminal","text":"<ul> <li>Select alma.icr.ac.uk from the list when you choose a remote connection and connect to the login node</li> <li>Now that you are connected to the login node, open the VSCode terminal and select the needed number of resource. An example is the following srun command: <pre><code>srun --pty --mem=10GB -c 2 -t 30:00:00 -p interactive bash\n</code></pre></li> </ul> <p>This will now create the connection to a compute node 01, since we asked for many resources. From here, you can run and test your code.</p> <p>OR:</p>"},{"location":"ides/remote/#connect-to-a-remote-host-compute-node","title":"Connect to a remote host: compute node","text":"<ul> <li>You will need to first log in to an interactive node on Alma with a terminal using: <pre><code>ssh username@alma.icr.ac.uk\nsrun --pty --mem=10GB -c 2 -t 30:00:00 -p interactive bash\n</code></pre></li> <li>in VSCode select \"Connect to host\" after clicking on the \"remote window\" icon in the far left bottom of the toolbar</li> <li>enter <code>ssh -J username@alma.icr.ac.uk node01</code></li> <li>Select the directory with your ssh key</li> </ul> <p>This will now create the connection. In the future you will be able to select this from the list when you choose a remote connection, but you will still need to first access a log in node from a terminal.</p> <p></p>"},{"location":"ides/vscode/","title":"VSCode","text":"<p>VSCode is a free open-source code editor developed by Microsoft. It has a built-in Git integration, debugging support, syntax highlighting, code completion, and more.  It is available for Windows, macOS, and Linux.</p> <p>VSCode is becoming the most popular code editor for software development and data science. It has a large number of extensions available for a wide range of languages and tools.</p>"},{"location":"ides/vscode/#installation","title":"Installation","text":"<p>To install VSCode, go to the VSCode website and download the installer for your operating system.</p>"},{"location":"ides/vscode/#vscode-tutorials","title":"VSCode Tutorials","text":"<ul> <li>Getting started with VSCode. This introduction to VSCode covers the basics of using the editor, using packages, using virtual environments by wiriting a simple \"roll the dice\" application.</li> <li>VSCode and python learning path modules. This is a collection of modules that cover the basics of using VSCode with python.</li> <li>Introduction to computer science with VSCode and python. This 19 hour course is a fully comprehensive introduction to computer science using VSCode and python./ It covers the basics of programming, data structures, algorithms, along with debugging, testing, and version control.</li> </ul>"},{"location":"ides/vscode/#extensions","title":"Extensions","text":"<p>VSCode has a large number of extensions available for a wide range of languages and tools. You can browse and install extensions from within VSCode by clicking on the Extensions icon in the Activity Bar on the side of the window.</p> <p>Some useful extensions for scientific software development include: - Python - R - Jupyter - Docker - Remote - Containers - Remote - WSL - Rainbow CSV - YAML</p>"},{"location":"ides/vscode/#configuration","title":"Configuration","text":"<p>VSCode has a large number of settings that can be configured to customise the editor to your preferences. You can access the settings by clicking on the gear icon in the bottom left of the window, or by pressing <code>Ctrl + ,</code>.</p> <p>You can also configure settings for specific languages and tools by creating a <code>settings.json</code> file in the <code>.vscode</code> directory in your project.</p>"},{"location":"ides/vscode/#remote-development","title":"Remote Development","text":"<p>VSCode has a number of extensions that allow you to develop code remotely on a different machine or in a container. This is useful if you are working on a machine with limited resources, or if you need to develop code in a different environment to your local machine.  </p> <p>The Remote-SSH extension allows you to connect to a remote machine over SSH and develop code on that machine. The Remote-Containers extension allows you to develop code in a container, and the Remote - WSL extension allows you to develop code in the Windows Subsystem for Linux.  </p> <p>To use VSCode for remote-ssh development follow the remote-ssh instructions.  </p>"},{"location":"ides/vscode/#debugging","title":"Debugging","text":"<p>VSCode has built-in support for debugging code written in a number of languages. You can configure debugging settings in the <code>.vscode/launch.json</code> file in your project, and then start a debugging session by clicking on the Debug icon in the Activity Bar on the side of the window.</p>"},{"location":"ides/vscode/#version-control","title":"Version Control","text":"<p>VSCode has built-in support for version control with Git. You can view and commit changes to your code, and push and pull changes to and from a remote repository, all from within the editor.</p>"},{"location":"ides/vscode/#integrated-terminal","title":"Integrated Terminal","text":"<p>VSCode has an integrated terminal that allows you to run commands and scripts from within the editor. You can open the terminal by clicking on the Terminal icon in the Activity Bar on the side of the window, or by pressing <code>Ctrl +</code>.</p>"},{"location":"ides/vscode/#keybindings","title":"Keybindings","text":"<p>VSCode has keyboard shortcuts that allow you to perform common tasks quickly and efficiently. You can view and customise the keybindings by clicking on the gear icon in the bottom left of the window, and then selecting Keyboard Shortcuts.</p>"},{"location":"ides/vscode/#themes","title":"Themes","text":"<p>VSCode has many themes available that allow you to customise the appearance of the editor. You can browse and install themes from within VSCode by clicking on the gear icon in the bottom left of the window, and then selecting Color Theme.</p>"},{"location":"ides/vscode/#snippets","title":"Snippets","text":"<p>VSCode has a large number of snippets available that allow you to quickly insert code templates for common tasks. You can browse and install snippets from within VSCode by clicking on the gear icon in the bottom left of the window, and then selecting User Snippets.</p>"},{"location":"ides/vscode/#workspaces","title":"Workspaces","text":"<p>VSCode has a feature called workspaces that allows you to group together related projects and open them all at once. You can create a workspace by clicking on the File menu, and then selecting Save Workspace As.</p>"},{"location":"ides/vscode/#further-reading","title":"Further reading","text":"<ul> <li>VSCode documentation</li> <li>VSCode tips and tricks</li> <li>VSCode keyboard shortcuts</li> <li>VSCode themes</li> <li>VSCode snippets</li> <li>VSCode workspaces</li> <li>VSCode extensions</li> <li>VSCode remote development</li> <li>VSCode debugging</li> <li>VSCode version control</li> <li>VSCode integrated terminal</li> <li>VSCode keybindings</li> <li>VSCode settings</li> </ul>"},{"location":"ides/wsl/","title":"Setting up with WSL","text":"<p>WSL can be installed from the windows store - try:  </p> <p>If you open it will bring up a command prompt and you enter credentials - the passowrd that you will use with sudo does not need to be particularly secure, it is not your main password.  </p> <p>Pin the prompt you get to your taskbar.  You may need to right-click and select ubuntu to open the correct shell.</p>"},{"location":"ides/wsl/#vscode","title":"VSCode","text":"<p>VSCode will install install itself. I suggest everythong goes into a dev directory, so: <pre><code>cd ~\ncode . #vscode will insteall itself\nmkdir dev\ncd dev\ncode .\n</code></pre></p>"},{"location":"ides/wsl/#conda","title":"Conda","text":"<p>Instructions mminiconda <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  \nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre></p>"},{"location":"ides/wsl/#gitlab-internal-or-external","title":"GitLab - internal or external","text":"<ol> <li>An ssh key for gitlab</li> </ol> <p>Create the ssh key on both your local computer and HPC with the following keygen command: <pre><code>ssh-keygen -t ecdsa -C comment\nchmod 600 ~/.ssh/id_ecdsa\n</code></pre> This will create a key file <code>~/.ssh/id_ecdsa.pub</code>. Copy the content of this file to gitlab (via web interface) in User/Preference/SSH Keys &gt; Add new key.</p> <ol> <li>ssh config Next, in your ~/.ssh/config file you will need to add the following (please ensure to replace  with your username): <pre><code>Host &lt;your_usernames&gt;.git.icr.ac.uk\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/id_ecdsa\n</code></pre>"},{"location":"ides/wsl/#vscode-extensions","title":"VSCode extensions","text":"<p>copilot ssh remote python</p>"},{"location":"ondemand/Rstudio/","title":"Running R Studio on Demand","text":"<p>To start the environment: <pre><code>module use /opt/software/easybuild/modules/all\n</code></pre></p> <p>it'll give you a newer R version you can load with  <pre><code>module load R/4.2.1-foss-2022a\n</code></pre> (<code>module load R</code> will also work but specifying the version means you'll get the same version if we update it later). </p>"},{"location":"ondemand/jupyter/","title":"Running Jupyter Notebook","text":""},{"location":"recipes/alphafold3/","title":"Running Alphafold3 on Alma","text":"<p>This guide outlines the steps required to run AlphaFold3 on the Alma HPC. AlphaFold3 is a powerful tool for predicting protein structures, but its setup and usage require careful preparation - including downloading model parameters, configuring input files, and leveraging Singularity.  Below, you\u2019ll find step-by-step instructions covering everything from requesting model access to preparing job submission scripts and interpreting results.</p> <p>Please note that AlphaFold3 is available only for non-commercial use, and access to its model parameters must be requested individually.</p> <p>IMPORTANT:</p> <ul> <li> <p>You\u2019ll need to fill out this form to request the model files. After submission, you\u2019ll receive an email from Google (typically within 2\u20133 days) containing a download link. Once you have the files, store them in your Scratch or RDS space on Alma.</p> </li> <li> <p>Please make sure you read the legal documents on the outputs terms of use, weights terms of use and weights prohibited use policy.</p> </li> <li> <p>Alphafold3 only works on GPUs, therefore, you will be charged GPU rates, which is currently \u2014 240p per GPU core per hour. </p> </li> </ul> <p>You can find a video tutorial on how to run Alphafold3 using the AlmaCookBook instructions on our YouTube channel. Please note the video is unlisted. </p> <p>All material has been adapted from the AlaphaFold3 GitHub repository.</p>"},{"location":"recipes/alphafold3/#downloading-alphafold-parameters","title":"Downloading AlphaFold Parameters","text":""},{"location":"recipes/alphafold3/#step-1-download-the-parameters","title":"Step 1: Download the Parameters","text":"<p>Once you receive the download link, click it to download the file <code>af3.bin.zst</code>.</p>"},{"location":"recipes/alphafold3/#step-2-moving-the-file-to-rds-or-scratch","title":"Step 2: Moving the File to RDS or Scratch","text":"<p>After downloading, you need to move the file to a suitable location on RDS or Scratch.</p> <p>You can do this in one of the following ways:</p> <ol> <li> <p>Via Mounted Fileshare (Recommended)   If RDS or Scratch is mounted on your local machine (e.g., via SMB), simply move the file using your file explorer (drag and drop).   For details, see 'The Alma fileshare' section in First Steps.</p> </li> <li> <p>Via <code>scp</code> in the Terminal   Use this method if you prefer the command line or the fileshare isn't mounted:</p> <pre><code>scp -J username@alma.icr.ac.uk af3.bin.zst username@node01:/data/scratch/some/dir/\n</code></pre> <p>Replace username and the path as appropriate.</p> </li> </ol>"},{"location":"recipes/alphafold3/#step-3-decompressing-the-parameter-file","title":"Step 3: Decompressing the Parameter File","text":"<p>Log into Alma following \"Connect to Alma and get an interactive partition\" section. Once the file is on the server, load the zstd module:</p> <p><pre><code>module load zstd\n</code></pre> Then decompress it to your desired directory (we recommend a folder like af3_model/):</p> <p><pre><code>zstd -d af3.bin.zst -o af3_model/af3.bin\n</code></pre> You can then use the path to your parameters <code>some/dir/af3_model</code> as <code>&lt;MODEL_PARAMETERS_DIR&gt;</code>.</p>"},{"location":"recipes/alphafold3/#connect-to-alma-and-get-an-interactive-partition","title":"Connect to Alma and get an interactive partition","text":"<pre><code>ssh username@alma.icr.ac.uk\nsrun --pty -t 12:00:00 -p interactive bash\n</code></pre>"},{"location":"recipes/alphafold3/#navigate-to-either-your-scratch-or-rds-directory","title":"Navigate to either your Scratch or RDS directory","text":"<pre><code>cd /data/scratch/&lt;your-username&gt;   # or /data/rds/&lt;your-username&gt;\n</code></pre>"},{"location":"recipes/alphafold3/#create-a-dedicated-directory-to-run-alphafold3-and-cd-into-it","title":"Create a dedicated directory to run Alphafold3 and <code>cd</code> into it","text":"<pre><code>mkdir alphafold3\ncd alphafold3\n</code></pre>"},{"location":"recipes/alphafold3/#singularity-image-and-execution-script","title":"Singularity Image and Execution Script","text":"<p>AlphaFold3 package is wrapped up in a singularity image that is made available on Alma here <code>/data/rds/DIT/SCICOM/SCRSE/shared/singularity/alphafold3.sif</code>. The code is invoked via the following python code.</p> <p>First copy the python code directly to the <code>alphafold3</code> directory:</p> <pre><code>wget -O run_alphafold.py https://raw.githubusercontent.com/google-deepmind/alphafold3/refs/heads/main/run_alphafold.py\n</code></pre>"},{"location":"recipes/alphafold3/#prepare-input-and-output-directories-and-files","title":"Prepare input and output directories and files","text":"<p>Running AlphaFold3 via calling <code>run_alphafold.py</code> requires mainly:</p> <ul> <li> <p>an input json file specifying the protein sequence</p> </li> <li> <p>an output folder where results will be stored.</p> </li> </ul> <p>Let's, create empty folders to store the input and output files:</p> <pre><code>mkdir af_input\nmkdir af_output\n</code></pre> <p>Navigate to the <code>af_input</code> folder and create your JSON file (e.g. <code>fold_input.json</code>) using your preferred editor (<code>nano</code>, <code>vi</code> or <code>vim</code>)</p> <pre><code>nano af_input/fold_input.json\n</code></pre> <p>Copy the following JSON object and save in the file. <pre><code>{\n  \"name\": \"2PV7\",\n  \"sequences\": [\n    {\n      \"protein\": {\n        \"id\": [\"A\", \"B\"],\n        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\"\n      }\n    }\n  ],\n  \"modelSeeds\": [1],\n  \"dialect\": \"alphafold3\",\n  \"version\": 1\n}\n</code></pre> For more details on the input format, check the official documentation.</p>"},{"location":"recipes/alphafold3/#running-alphafold3-via-a-bash-script","title":"Running AlphaFold3 via a Bash Script","text":"<p>To run AlphaFold3 on Alma, you\u2019ll typically create and submit a bash script that sets up the computational environment and launches the job with your specified resources (CPUs, GPUs, time, etc.). This script wraps the Singularity container execution and points to the right input, output, and model directories.</p> <p>Create a bash script (e.g.,<code>af3-test.sh</code>) in <code>alphafold3</code> directory with the following content and replace  with the actual path where you stored <code>AlphaFold3 model parameters</code>. <pre><code>#!/bin/bash\n#SBATCH --job-name=af3\n#SBATCH --partition=gpu\n#SBATCH --output=af_output/af3.out\n#SBATCH --error=af_output/af3.err\n#SBATCH --time=00:20:00\n#SBATCH --cpus-per-task=20 \n#SBATCH --gres=gpu:1\n\nAF_DIR=$(pwd)\n\nsingularity exec \\\n    --nv \\\n    --bind $AF_DIR/af_input:/root/af_input \\\n    --bind $AF_DIR/af_output:/root/af_output \\\n    --bind &lt;MODEL_PARAMETERS_DIR&gt;:/root/models \\\n    --bind /data/reference-data/alphafold_db:/root/public_databases \\\n    /data/rds/DIT/SCICOM/SCRSE/shared/singularity/alphafold3.sif \\\n    python run_alphafold.py \\\n    --flash_attention_implementation=xla \\\n    --json_path=/root/af_input/fold_input.json \\\n    --model_dir=/root/models \\\n    --db_dir=/root/public_databases \\\n    --output_dir=/root/af_output\n</code></pre> <p>Where:</p> <ul> <li> <p><code>$AF_DIR</code> - is the path to the directory where you are running AlphaFold3 from </p> </li> <li> <p><code>&lt;MODEL_PARAMETERS_DIR&gt;</code> - path to your downloaded model parameters</p> </li> <li> <p><code>/data/reference-data/alphafold_db</code> - path to the protein database in ICR shared data folder</p> </li> </ul> <p>IMPORTANT: </p> <ul> <li> <p><code>/data/rds/DIT/SCICOM/SCRSE/shared/singularity/alphafold3.sif</code> is a path that contains the AlphaFold3 singularity image to be used by everyone in the institute. If you would like to build one for your own machine, follow the next section on \"What you need to build Alphafold3 Docker image yourself - Important Notes\".</p> </li> <li> <p>Depending on the length and complexity of your protein, you need to specify an appropriate number of GPUs, CPUs and time for your job to complete. </p> </li> <li> <p><code>--flash_attention_implementation=xla</code> flag is very important to run the image successfully on Alma. Do not remove it.</p> </li> </ul> <p>There are various flags that you can pass to the run_alphafold.py command, to list them all run python run_alphafold.py --help. Two fundamental flags that control which parts AlphaFold3 will run are:</p> <p><code>--run_data_pipeline</code> (defaults to true): whether to run the data pipeline, i.e. genetic and template search. This part is CPU-only, time-consuming and could be run on a machine without a GPU.</p> <p><code>--run_inference</code> (defaults to true): whether to run the inference. This part requires a GPU.</p>"},{"location":"recipes/alphafold3/#how-to-use-the-script","title":"How to use the script?","text":"<ol> <li> <p>Make it executable: <pre><code>chmod +x af3-test.sh\n</code></pre></p> </li> <li> <p>Submit the job to Alma scheduler: <pre><code>sbatch af3-test.sh\n</code></pre></p> </li> </ol> <p>After submitting your job with <code>sbatch af3-test.sh</code>, you can check its status using:</p> <p><pre><code>squeue -u $USER\n</code></pre> This command lists all your running and queued jobs. Look for the job named af3 to confirm it\u2019s running.</p> <p>Note: Depending on your protein sequence and resources requested, the job may take a considerable time to complete</p>"},{"location":"recipes/alphafold3/#checking-the-outputs","title":"Checking the Outputs","text":"<p>Once the job finishes, outputs will be saved in the <code>af_output</code> directory:</p> <ul> <li>af3.out \u2014 standard output log from the job</li> <li>af3.err \u2014 error log, if any</li> </ul> <p>If the job run is successful, your <code>af3.out</code> file will include \"Done running 1 fold jobs.\" message. The number of folds depends on the number of structures you put in your JSON file. You will also have a folder named after your protein (e.g., <code>2PV7</code>), containing the predicted structure files and related results.</p> <p>Here\u2019s an example of what the output directory might look like:</p> <p></p> <p>The output documentation can be found here.</p>"},{"location":"recipes/alphafold3/#alphafold3-image-hardware-specifications","title":"Alphafold3 Image - Hardware Specifications","text":"<p>IMPORTANT:</p> <ul> <li>This is applicable if you want to run Alphafild3 on your own machine. Please use the shared singularity image if you are running Alphafold3 on Alma. </li> </ul>"},{"location":"recipes/alphafold3/#1-use-appropriate-hardware","title":"1. Use appropriate hardware.","text":"<p>Docker image only builds on appropriate hardware. We used the following workstation to build one for Alma:</p> <ul> <li>CPU: Intel Xeon Gold 5118 CPU 2.30GHz (Sockets: 1, Cores: 12, Threads: 24)</li> <li>GPU: Nvidia Quadro GV100 (32G)</li> <li>RAM: 128G</li> </ul>"},{"location":"recipes/alphafold3/#2-optional-change-the-original-docker-file-if-using-cuda-capability-7x-gpus","title":"2. [OPTIONAL] Change the original Docker file if using CUDA Capability 7.x GPUs","text":"<p>CUDA Compute Capability 7.x GPUs have limited numerical accuracy and performance (more here). Since Alma uses such GPUs, you will need to modify a few lines in the Dockerfile.</p> <p>Specifically, the original Dockerfile contains the following lines near the end:erfile. </p> <pre><code># To work around a known XLA issue causing the compilation time to greatly\n# increase, the following environment variable setting XLA flags must be enabled\n# when running AlphaFold 3. Note that if using CUDA capability 7 GPUs, it is\n# necessary to set the following XLA_FLAGS value instead:\n# ENV XLA_FLAGS=\"--xla_disable_hlo_passes=custom-kernel-fusion-rewriter\"\n# (no need to disable gemm in that case as it is not supported for such GPU).\nENV XLA_FLAGS=\"--xla_gpu_enable_triton_gemm=false\"\n# Memory settings used for folding up to 5,120 tokens on A100 80 GB.\nENV XLA_PYTHON_CLIENT_PREALLOCATE=true\nENV XLA_CLIENT_MEM_FRACTION=0.95\n</code></pre> <p>Since Alma has CUDA capability 7 GPU card, you need to:</p> <pre><code># Uncomment this line\nENV XLA_FLAGS=\"--xla_disable_hlo_passes=custom-kernel-fusion-rewriter\"\n# Comment out this line\n# ENV XLA_FLAGS=\"--xla_gpu_enable_triton_gemm=false\"\n</code></pre> <p>IMPORTANT: Software Specification </p> <p>Docker is not supported on Alma. If you\u2019re working on a system without Docker, you can use Singularity instead, which is available on Alma. In this case, you\u2019ll need to push your Docker image to a remote registry (such as DockerHub) and then pull it as a Singularity image.</p>"},{"location":"recipes/matlab-python/","title":"Running Matlab code from Python on Alma","text":"<p><code>Maintained by: Rachel Alcraft. Last updated: 31st July 2024</code></p> <p>Running MatLab code from python requires the installation of an additional package called <code>matlab.engine</code>. This package is installed using pip system into a mamba environment.</p> <p>We need to control the version to be compatible with Alma - pythons 3.9, 3.10 and 3.11 are compatible with the latest Alma installation of MatLab R2023b, and compatible with matlabengine==23.2.1</p> <p>If you want to use an earlier python version, you will need to install an earlier version of the MatLab engine. For compatibility with python 3.8 (eg ig=f you are using a cuda gpu) you will need to install matlabengine==9.14.7</p>"},{"location":"recipes/matlab-python/#get-setup-on-alma","title":"Get setup on Alma","text":""},{"location":"recipes/matlab-python/#log-onto-an-interactive-session-on-alma","title":"Log onto an interactive session on Alma","text":"<pre><code>ssh username@alma.icr.ac.uk\nsrun --pty --mem=10GB -c 1 -t 30:00:00 -p interactive bash\n</code></pre>"},{"location":"recipes/matlab-python/#create-a-mamba-environment-to-work-in","title":"Create a mamba environment to work in","text":"<pre><code>mamba create -n my-matlab python=3.11\nmamba activate my-matlab\n</code></pre>"},{"location":"recipes/matlab-python/#add-the-library-path","title":"add the library path","text":"<pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/software/applications/MATLAB/R2023b/bin/glnxa64\n</code></pre>"},{"location":"recipes/matlab-python/#install-the-matlab-engine","title":"Install the MatLab engine","text":"<pre><code>python -m pip install --upgrade pip\npython -m pip install matlabengine==23.2.1\n</code></pre>"},{"location":"recipes/matlab-python/#check-that-the-required-python-and-matlab-versions-were-correctly-installed","title":"Check that the required python and MatLab versions were correctly installed","text":"<pre><code>which python\npython --version\npython -m pip show matlabengine\npython -c \"import sys; print('\\n'.join(sys.path))\"\n</code></pre>"},{"location":"recipes/matlab-python/#start-the-matlab-engine-and-test-it","title":"Start the MatLab engine and test it","text":""},{"location":"recipes/matlab-python/#open-the-python-shell","title":"Open the python shell","text":"<pre><code>python\n</code></pre>"},{"location":"recipes/matlab-python/#run-the-following-paste-it-in-line-by-line","title":"Run the following: paste it in line by line","text":"<p><pre><code>print(\"Starting MatLab Test\")\nimport matlab.engine\neng = matlab.engine.start_matlab()\nx = 4.0\neng.workspace['y'] = x\na = eng.eval('sqrt(y)')\nprint(\"If a == 2.0 then it works!\")\nprint(\"a =\",a)\n</code></pre> You can now create scripts that use MatLab and run on Alma when using this mamba environment. Further resources for the installation and use of the MatLab engine can be found below.</p>"},{"location":"recipes/matlab-python/#resources","title":"Resources","text":""},{"location":"recipes/matlab-python/#installation","title":"Installation","text":"<ul> <li>https://pypi.org/project/matlabengine/23.2.1/ </li> <li>https://uk.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html </li> </ul>"},{"location":"recipes/matlab-python/#use","title":"Use","text":"<ul> <li>https://uk.mathworks.com/help/matlab/matlab_external/get-started-with-matlab-engine-for-python.html</li> <li>https://uk.mathworks.com/help/matlab/matlab_external/start-the-matlab-engine-for-python.html</li> <li>https://uk.mathworks.com/help/matlab/matlab_external/use-the-matlab-engine-workspace-in-python.html</li> </ul>"},{"location":"recipes/model/","title":"model","text":"<p>This code is taken and adapted from the website  benchmarking-cpu-and-gpu-performance-with-tensorflow </p> <pre><code>\"\"\"\nThis code is taken and adapted from the website \nhttps://www.analyticsvidhya.com/blog/2021/11/benchmarking-cpu-and-gpu-performance-with-tensorflow/\n\"\"\"\nimport datetime\ndef get_now():\n    return datetime.datetime.now().strftime('%d-%m-%Y %H:%M')\n\n# 0. Check input is CPU or GPU and time them\nimport sys\nname_of_script = sys.argv[0]\nis_cpu = True\nif len(sys.argv) &gt; 1:\n    if sys.argv[1].lower() == 'gpu':\n        is_cpu = False\n\nprint(\"Is CPU = \", is_cpu)\n\nstarttime = datetime.datetime.now()\nprint(\"----------------------------------------------------\")\nprint(\"Starting at:\", get_now())\n\n\n# 1. Import \u2013  necessary modules and the dataset.\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(\"Loading dataset at\",get_now())\n(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\nprint(\"...loaded at\",get_now())\n\n# 2. Perform  Eda \u2013 check data and labels shape:\n# checking images shape\nX_train.shape, X_test.shape\n# display single image shape\nX_train[0].shape\n# checking labels\ny_train[:5]\n\n# 3. Apply Preprocessing: Scaling images(NumPy array) by 255 and One-Hot Encoding labels to represent all categories as 0, except  1 for the actual label in \u2018float32.\u2019\n# scaling image values between 0-1\nX_train_scaled = X_train/255\nX_test_scaled = X_test/255\n# one hot encoding labels\ny_train_encoded = keras.utils.to_categorical(y_train, num_classes = 10, dtype = 'float32')\ny_test_encoded = keras.utils.to_categorical(y_test, num_classes = 10, dtype = 'float32')\n\n# 4. Model Building: A fn to build a neural network with architecture as below with compiling included :\n\ndef get_model():\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(32,32,3)),\n        keras.layers.Dense(3000, activation='relu'),\n        keras.layers.Dense(1000, activation='relu'),\n        keras.layers.Dense(10, activation='sigmoid')    \n    ])\n    model.compile(optimizer='SGD',\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n    return model\n\n\n# 5. Training: Train for ten epochs which verbose = 0, meaning no logs.\nif is_cpu:\n    with tf.device('/CPU:0'):\n        print(\"Running model oin CPUs at\",get_now())\n        model_cpu = get_model()\n        model_cpu.fit(X_train_scaled, y_train_encoded, epochs = 2)\n        print(\"...run at\",get_now())\nelse: \n    with tf.device('/GPU:0'):\n        print(\"Running model on GPUs at\",get_now())\n        model_gpu = get_model()\n        model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 2)\n        print(\"...run at\",get_now())\n\nendtime = datetime.datetime.now()\ndiff = endtime - starttime\nprint('Job took: ', diff.days, diff.seconds, diff.microseconds)\n</code></pre>"},{"location":"recipes/muspan/","title":"A Basic Loading of a MuSpAn Dataset","text":"<p>In order to use MuSpAn you need to secure a license. Request the code from: https://www.muspan.co.uk/get-the-code and you will receieve an  email with the username and password needed for the install.</p> <p>We recommend using conda or mamba to install the muspan package.  The following will create a new conda environment called <code>muspan-env</code> and install  the package in an environment you can use in a jupyter notebook:</p> <pre><code>conda create -y -n muspan-env -c conda-forge python=3.12 \nconda activate muspan-env\npython -m pip install --upgrade pip\npython -m pip install https://docs.muspan.co.uk/code/latest.zip\n# follow the username and password prompts\nconda install jupyterlab ipykernel ipywidgets\npython -m ipykernel install --user --name python_muspan --display-name \"Python Muspan 2025\"\n</code></pre> <p>In python verify you can access the MuSpAn package and a sample dataset:</p> <pre><code>import pandas as pd\nimport os\nimport muspan as ms\nfile_path = os.path.dirname(ms.datasets.__file__) + '/data/Bull_2024_mouse_colon.csv'\nprint(file_path)\ndf = pd.read_csv(file_path)\nprint(df.head())\n</code></pre>"},{"location":"recipes/tensorflow-gpu/","title":"Running TensorFlow python code on an Alma GPU","text":"<p><code>Maintained by: Rachel Alcraft. Last updated: 1st August 2024</code></p> <p>TensorFlow is a popular machine learning library that can be used to train and run deep learning models. TensorFlow can be run on a GPU to speed up training and inference. This guide will show you how to run TensorFlow code on an Alma GPU.</p> <p>Getting the versions to match is tricky, this recipe works on 11th July 2024 when CUDA 11.1 on Alma at /opt/software/compilers/cuda/11.1 is the most recent CUDA version.  The versions of TensorFlow and CUDA are changing rapidly so you may need to adjust the versions to get them to work together.</p>"},{"location":"recipes/tensorflow-gpu/#create-the-mamba-environment","title":"Create the mamba environment","text":""},{"location":"recipes/tensorflow-gpu/#get-setup-on-alma","title":"Get setup on Alma","text":"<pre><code>ssh username@alma.icr.ac.uk\nsrun --pty --mem=10GB -c 1 -t 30:00:00 -p interactive bash\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#create-a-mamba-environment-to-work-in-for-compatible-versions","title":"Create a mamba environment to work in for compatible versions:","text":"<p>We have cuda 11.1 installed so we need: Versions compatibility link </p> Version Python version Compiler Build tools cuDNN CUDA tensorflow-2.4.0 3.6-3.8 GCC 7.3.1 Bazel 3.1.0 8.0 11.0 <pre><code>mamba create -n my-tensorflow -c conda-forge python=3.7 cudatoolkit=11.2.2 cudnn=8.1.0.77\nmamba activate my-tensorflow\nmamba install conda-forge::tensorflow\nmamba install conda-forge::tensorflow-gpu\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#check-that-the-required-versions-of-python-and-tensorflow-were-correctly-installed","title":"Check that the required versions of python and TensorFlow were correctly installed","text":"<pre><code>which python\npython --version\npython -m pip show tensorflow\npython -c \"import sys; print('\\n'.join(sys.path))\"\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#navigate-somewhere-sensible","title":"Navigate somewhere sensible","text":"<pre><code>mkdir /path/to/your/code\ncd /path/to/your/code\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#create-a-python-script-to-run-tensorflow-code","title":"Create a python script to run TensorFlow code","text":"<p>Create a file <code>/path/to/your/code/python_script.py</code> with the following content: <pre><code>import sys\nimport tensorflow as tf\n\nprint(\"Starting Cuda-GPU-TensorFlow Test\")\nprint('\\n'.join(sys.path))\nprint(tf.__version__)\n\nfrom tensorflow.python.client import device_lib\nlocal_device_protos = device_lib.list_local_devices()\nnum_gpus = 0\nfor x in local_device_protos:    \n    print(x)\n    if 'GPU' in x.device_type:\n        num_gpus += 1\nprint('Num GPUs Available: ', num_gpus)\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#create-a-batch-script-to-run-the-python-script","title":"Create a batch script to run the python script","text":"<p>Create a file <code>/path/to/your/code/sbatch_script.sh</code> with the following content: <pre><code>#!/bin/sh\n#SBATCH -J \"tf_test\"\n#SBATCH -p gpu\n#SBATCH -e tf_tst.err\n#SBATCH -o tf_tst.out\n#SBATCH -t 12:00:00\n#SBATCH --gres=gpu:1\npython ./python_script.py\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#submit-the-batch-script-to-the-queue","title":"Submit the batch script to the queue","text":"<p>You can submit the batch job to slurmn and it will be run on a GPU node. <pre><code>chmod +x sbatch_script.sh\nsbatch sbatch_script.sh\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#monitor-the-job","title":"Monitor the job","text":"<p>Check the job is running and on a gpu node: <pre><code>squeue -u $USER\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#check-the-output","title":"Check the output","text":"<p>The output from the print statements in python are diverted to the file <code>tf_tst.out</code> and any errors are diverted to <code>tf_tst.err</code>. You can check the output with the following command: <pre><code>cat tf_tst.out\ncat tf_tst.err\n</code></pre> The physical devices check that there is access to a GPU node. Note if you run the python script directly from the interactive node there will not be a GPU available.</p>"},{"location":"recipes/tensorflow-gpu/#test-the-gpu-by-training-a-model","title":"Test the GPU by training a model","text":"<p>You can now create some more serious TensorFlow code and submit it to the queue to run on a GPU node that will be run on a GPU. You can check the speed on the GPU node compared to the interactive node. Create another file <code>/path/to/your/code/python_train.py</code> with the following content:</p> <p>We are using the test training code for CPU vs GPU from this site: Benchmarking CPU And GPU Performance With Tensorflow</p>"},{"location":"recipes/tensorflow-gpu/#first-add-some-more-modules-to-the-mamba-environment","title":"First add some more modules to the mamba environment:","text":"<pre><code>mamba install conda-forge::matplotlib\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#create-a-python-file","title":"Create a python file","text":"<p>Create a file <code>model.py</code> with the following content in this file </p> <p>Test the file works from the interactive node: <pre><code>python model.py\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#create-2-new-sbatch-files","title":"Create 2 new sbatch files","text":"<p>Create a file <code>sbatch_train_cpu.sh</code> and <code>sbatch_train_cpu.sh</code> with the following content: <code>sbatch_train_cpu.sh</code> <pre><code>#!/bin/sh\n#SBATCH -J \"cpu_train\"\n#SBATCH -p compute\n#SBATCH -e cpu_train.err\n#SBATCH -o cpu_train.out\n#SBATCH -t 12:00:00\npython ./model.py cpu\n</code></pre> <code>sbatch_train_gpu.sh</code> <pre><code>#!/bin/sh\n#SBATCH -J \"gpu_train\"\n#SBATCH -p gpu\n#SBATCH -e gpu_train.err\n#SBATCH -o gpu_train.out\n#SBATCH -t 12:00:00\n#SBATCH --gres=gpu:1\npython ./model.py gpu\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#run-them-both","title":"Run them both","text":"<pre><code>chmod +x sbatch_train_cpu.sh\nchmod +x sbatch_train_gpu.sh\nsbatch sbatch_train_cpu.sh\nsbatch sbatch_train_gpu.sh\n</code></pre>"},{"location":"recipes/tensorflow-gpu/#monitor-the-jobs","title":"Monitor the jobs","text":"<p>Check the job is running and on a gpu node: <pre><code>squeue -u $USER\n</code></pre></p>"},{"location":"recipes/tensorflow-gpu/#check-the-output_1","title":"Check the output","text":"<p>The output from the print statements in python are diverted to the file <code>tf_tst.out</code> and any errors are diverted to <code>tf_tst.err</code>. You can check the output with the following command: <pre><code>cat cpu_train.out\ncat cpu_train.err\ncat gpu_train.out\ncat gpu_train.err\n</code></pre> When I run it the GPU is 10x faster: 232 seconds for the CPU and 26 seconds for the GPU.</p>"},{"location":"shiny-apps/introduction/","title":"Introduction to Shiny Apps","text":""},{"location":"shiny-apps/introduction/#what-is-shiny","title":"What is Shiny?","text":"<p>Shiny is an R package that makes it easy to build interactive web applications directly from R. With Shiny, you can create dashboards, data analysis tools, and visual reports that update in real time as users interact with the app. It's built on top of R and leverages reactive programming to respond instantly to user input.</p>"},{"location":"shiny-apps/introduction/#why-would-you-use-the-shiny-framework","title":"Why would you use the Shiny framework?","text":"<p>Shiny is ideal for anyone who wants to share data insights or analysis results in an accessible, interactive format \u2014 especially in research, business, and data science. Instead of sending static plots or spreadsheets, you can deliver an app where others can explore different scenarios, filter data, run models, and view results instantly. It\u2019s perfect when you need quick, custom tools without the overhead of full-stack web development.</p>"},{"location":"shiny-apps/shiny-r/","title":"Shiny R Apps","text":""},{"location":"shiny-apps/shiny-r/#what-are-they-made-of","title":"What are they made of?","text":"<p>A Shiny app is built in two main parts:</p> <ol> <li>UI (User Interface)</li> <li>Defines how the app looks \u2014 layout, inputs (buttons, sliders, file uploads), outputs (plots, tables, text).</li> <li>You specify what the user sees and interacts with.</li> <li> <p>Example elements: <code>actionButton()</code>, <code>sliderInput()</code>, <code>plotOutput()</code>, <code>tableOutput()</code>.</p> </li> <li> <p>Server</p> </li> <li>Contains the logic and calculations behind the app.</li> <li>Defines how the app responds to user inputs.</li> <li>Creates reactive outputs using <code>render*()</code> functions (e.g., <code>renderPlot()</code>, <code>renderTable()</code>).</li> <li>Uses reactive expressions to update outputs automatically when inputs change.</li> </ol> <p>While it's possible to include both the UI and server components in a single app.R script, I recommend separating the UI, server, and any additional analysis code into distinct scripts. This is the structure I will follow throughout the rest of this tutorial.</p>"},{"location":"shiny-apps/shiny-r/#how-do-ui-and-server-communicate","title":"How do UI and Server communicate?","text":"<ul> <li>Each UI output element (like <code>plotOutput(\"plot1\")</code>), has an ID (i.e., <code>plot1</code>) and corresponds to a server-side render function (<code>output$plot1 &lt;- renderPlot({...})</code>).</li> <li><code>output$plot1 &lt;- renderPlot({...})</code> means \"Draw a plot and display it in the UI where the placeholder called <code>plotOutput(\"plot1\")</code> is\".</li> <li>Inputs from the UI (like <code>input$slider1</code>) are available in the server code to control input behavior or data processing.</li> <li>The reactive programming model ensures the server updates outputs automatically when relevant inputs change.</li> </ul>"},{"location":"shiny-apps/shiny-r/#some-commonly-used-libraries","title":"Some commonly used libraries:","text":"<ul> <li>shiny \u2013 Core framework for building interactive web apps in R. Handles reactivity between UI inputs and server logic.</li> <li>shinydashboard \u2013 Provides a clean, structured layout system for dashboards using components like headers, sidebars, and value boxes.</li> <li>ggplot2 \u2013 Powerful plotting library based on the grammar of graphics. Used to create customizable plots like scatterplots, bar charts, and histograms.</li> <li>dplyr \u2013 Data manipulation toolkit with intuitive functions (<code>filter()</code>, <code>mutate()</code>, <code>summarise()</code>) for transforming and summarising datasets efficiently.</li> <li>DT \u2013 Enables interactive tables (search, sort, paginate) in Shiny apps.</li> </ul>"},{"location":"shiny-apps/shiny-r/#some-commonly-used-functions","title":"Some commonly used functions","text":""},{"location":"shiny-apps/shiny-r/#overview","title":"Overview","text":"<p>This guide covers the most commonly used functionalities in Shiny R applications. Shiny follows a reactive programming model where the UI (User Interface) defines what users see, and the server function defines how the app responds to user interactions.</p>"},{"location":"shiny-apps/shiny-r/#basic-structure","title":"Basic Structure","text":"<p>UI and server are the two main components:</p> <pre><code>library(shiny)\n\nui &lt;- fluidPage(\n  # UI elements go here\n)\n\nserver &lt;- function(input, output, session) {\n  # Server logic goes here\n}\n\nshinyApp(ui = ui, server = server)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#essential-server-functions","title":"Essential Server Functions","text":""},{"location":"shiny-apps/shiny-r/#the-req-function","title":"The <code>req()</code> Function","text":"<p>Use <code>req()</code> to ensure code only runs when required inputs exist:</p> <pre><code># Don't run unless rawData exists and is not NULL\nreq(rawData())\n\n# Don't run unless multiple conditions are met\nreq(input$file, input$column, rawData())\n</code></pre>"},{"location":"shiny-apps/shiny-r/#reactive-values-and-functions","title":"Reactive Values and Functions","text":"<pre><code># Store data that can change over time\nrawData &lt;- reactiveVal(NULL)\nprocessedData &lt;- reactiveVal(NULL)\n\n# Create reactive expressions (computed values that update automatically)\nfilteredData &lt;- reactive({\n  req(rawData())\n  # Your filtering logic here\n  filter(rawData(), column &gt; input$threshold)\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#observer-functions","title":"Observer Functions","text":"<pre><code># observeEvent: Responds to specific input changes\nobserveEvent(input$button, {\n  # Code runs when button is clicked\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#file-upload-and-data-loading","title":"File Upload and Data Loading","text":""},{"location":"shiny-apps/shiny-r/#ui-components","title":"UI Components","text":"<pre><code># Choosing file to uploda\nfileInput(\"upload\", \n          \"Choose CSV File\", \n          accept = c(\".csv\", \".txt\"),\n          multiple = FALSE),\n# Action button to upload the data \nactionButton(\"loadBtn\", \"Load Data\")\n</code></pre>"},{"location":"shiny-apps/shiny-r/#server-logic","title":"Server Logic","text":"<pre><code>observeEvent(input$loadBtn, {\n  req(input$upload)\n\n  # Show loading message\n  showNotification(\"Loading data...\", type = \"message\")\n\n  tryCatch({\n    # Read the uploaded file\n    data &lt;- read.csv(input$upload$datapath, \n                     header = TRUE, \n                     stringsAsFactors = FALSE)\n\n    # Store in reactive value\n    rawData(data)\n\n    # Success message\n    showNotification(\"Data loaded successfully!\", type = \"message\")\n\n  }, error = function(e) {\n    showNotification(paste(\"Error loading file:\", e$message), type = \"error\")\n  })\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#user-interface-elements","title":"User Interface Elements","text":""},{"location":"shiny-apps/shiny-r/#buttons","title":"Buttons","text":"<pre><code># UI\nactionButton(\"analyzeBtn\", \"Analyze Data\")\n\n# Server\nobserveEvent(input$analyzeBtn, {\n  req(rawData())\n\n  # Your analysis code here\n  data &lt;- rawData()\n\n  # Example analysis\n  analyzed_data &lt;- data %&gt;%\n    mutate(mean_age = mean(Age, na.rm = TRUE),\n           age_category = ifelse(Age &gt; mean_age, \"Above Average\", \"Below Average\"))\n\n  processedData(analyzed_data)\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#numeric-inputs-and-sliders","title":"Numeric Inputs and Sliders","text":"<pre><code># UI\nnumericInput(\"threshold\", \"Set threshold:\", \n             value = 50, min = 0, max = 100, step = 1),\nsliderInput(\"range\", \"Select range:\", \n            min = 0, max = 100, value = c(25, 75)),\nsliderInput(\"bins\", \"Number of bins:\", \n            min = 10, max = 50, value = 30)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#selection-inputs","title":"Selection Inputs","text":"<pre><code># UI\n# Dropdown\nselectInput(\"column\", \"Choose column:\", \n            choices = NULL,  # Will be updated from server\n            selected = NULL),\n#  Radio buttons\nradioButtons(\"plot_type\", \"Plot type:\",\n             choices = list(\"Histogram\" = \"hist\",\n                           \"Boxplot\" = \"box\",\n                           \"Scatter\" = \"scatter\"),\n             selected = \"hist\"),\n# Multiple checkboxes\ncheckboxGroupInput(\"variables\", \"Select variables:\",\n                   choices = NULL)  # Updated from server\n\n# Server - Update choices dynamically\nobserve({\n  req(rawData())\n\n  # Update column choices based on loaded data\n  updateSelectInput(session, \"column\",\n                    choices = names(rawData()),\n                    selected = names(rawData())[1])\n\n  # Update variable choices for numeric columns only\n  numeric_cols &lt;- names(select_if(rawData(), is.numeric))\n  updateCheckboxGroupInput(session, \"variables\",\n                          choices = numeric_cols)\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#date-inputs","title":"Date Inputs","text":"<pre><code># UI\ndateInput(\"start_date\", \"Start date:\", \n    value = Sys.Date() - 30),\ndateRangeInput(\"date_range\", \"Date range:\",\n    start = Sys.Date() - 30,\n    end = Sys.Date())\n</code></pre>"},{"location":"shiny-apps/shiny-r/#output-functions","title":"Output Functions","text":""},{"location":"shiny-apps/shiny-r/#tables","title":"Tables","text":"<pre><code># UI\ntableOutput(\"static_table\"),     # Static table\nDT::dataTableOutput(\"interactive_table\")  # Interactive table (requires DT package)\n\n# Server\n# Static table\noutput$static_table &lt;- renderTable({\n  req(processedData())\n  head(processedData(), 10)\n}, striped = TRUE, hover = TRUE)\n\n# Interactive table - with pagination \noutput$interactive_table &lt;- DT::renderDataTable({\n  req(processedData())\n  processedData()\n}, options = list(pageLength = 10, scrollX = TRUE))\n</code></pre>"},{"location":"shiny-apps/shiny-r/#text-and-summaries","title":"Text and Summaries","text":"<pre><code># UI\nverbatimTextOutput(\"summary\"),\ntextOutput(\"status\")\n\n# Server\noutput$summary &lt;- renderPrint({\n  req(rawData())\n  # Table summary example\n  summary(rawData())\n})\n\noutput$status &lt;- renderText({\n  req(rawData())\n  # Text output sexample\n  paste(\"Data loaded with\", nrow(rawData()), \"rows and\", ncol(rawData()), \"columns\")\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#plots","title":"Plots","text":"<pre><code># UI\nuiOutput(\"column_selector\"),\nselectInput(\"plot_type\", \"Choose column:\", \n            choices = c(\"hist\", \"box\"),\n            selected = NULL),\nplotOutput(\"plot1\", \n        height = \"400px\")\n\n# Server\noutput$plot1 &lt;- renderPlot({\n    # Choose what data to require\n    req(rawData(), input$column)\n    # Choose what data to use\n    data &lt;- rawData()\n\n    if (input$plot_type == \"hist\") {\n    ggplot(data, aes_string(x = input$column)) +\n        geom_histogram(bins = input$bins, fill = \"skyblue\", alpha = 0.7) +\n        theme_minimal() +\n        labs(title = paste(\"Histogram of\", input$column))\n\n    } else if (input$plot_type == \"box\") {\n    ggplot(data, aes_string(y = input$column)) +\n        geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +\n        theme_minimal() +\n        labs(title = paste(\"Boxplot of\", input$column))\n  }\n})\n\noutput$column_selector &lt;- renderUI({  \n    # Choose what data to use\n    data &lt;- rawData()\n    if (is.null(data)) return(NULL)  # Wait for data    \n    numeric_cols &lt;- names(data)[sapply(data, is.numeric)]  # Choose only numeric columns\n\n    if (length(numeric_cols) == 0) {\n        return(p(\"No numeric columns found in data.\"))\n    }\n\n    selectInput(\"column\", \"Select Column:\", choices = numeric_cols)\n})\n</code></pre>"},{"location":"shiny-apps/shiny-r/#layout-options","title":"Layout Options","text":""},{"location":"shiny-apps/shiny-r/#basic-layout","title":"Basic Layout","text":"<pre><code># Simple fluid layout\nui &lt;- fluidPage(\n  titlePanel(\"My Shiny App\"),\n\n  sidebarLayout(\n    sidebarPanel(\n      # Input controls go here\n      width = 4\n    ),\n\n    mainPanel(\n      # Output displays go here\n      width = 8\n    )\n  )\n)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#dashboard-layout","title":"Dashboard Layout","text":"<pre><code>library(shinydashboard)\n\nui &lt;- dashboardPage(\n  dashboardHeader(title = \"My Dashboard\"),\n\n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Data Upload\", tabName = \"upload\", icon = icon(\"upload\")),\n      menuItem(\"Analysis\", tabName = \"analysis\", icon = icon(\"chart-line\")),\n      menuItem(\"Results\", tabName = \"results\", icon = icon(\"table\"))\n    )\n  ),\n\n  dashboardBody(\n    tabItems(\n        tabItem(tabName = \"upload\",\n        # Upload content\n        ),\n        tabItem(tabName = \"analysis\",\n        # Analysis content\n        ),\n        tabItem(tabName = \"results\",\n        # Results content\n        )\n    )\n    )\n)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#tabbed-layout","title":"Tabbed Layout","text":"<pre><code>ui &lt;- fluidPage(\n  titlePanel(\"Multi-tab Application\"),\n    tabsetPanel(\n        tabPanel(\"Data\", \n        # Data upload and preview\n        ),\n        tabPanel(\"Analysis\",\n        # Analysis controls and outputs\n        ),\n        tabPanel(\"Visualization\",\n        # Plots and charts\n        )\n    )\n)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#advanced-features","title":"Advanced Features","text":""},{"location":"shiny-apps/shiny-r/#conditional-panels","title":"Conditional Panels","text":"<pre><code># UI\nconditionalPanel(\n    condition = \"input.show_advanced == true\",\n    # UI elements that only show when checkbox is checked\n    numericInput(\"advanced_param\", \"Advanced parameter:\", value = 1)\n),\n\ncheckboxInput(\"show_advanced\", \"Show advanced options\", value = FALSE)\n</code></pre>"},{"location":"shiny-apps/shiny-r/#notifications","title":"Notifications","text":"<pre><code># Show different types of notifications\nshowNotification(\"Success!\", type = \"message\")\nshowNotification(\"Warning!\", type = \"warning\")\nshowNotification(\"Error occurred!\", type = \"error\")\nshowNotification(\"Information\", type = \"message\")\n</code></pre>"},{"location":"shiny-apps/shiny-r/#best-practices","title":"Best Practices","text":"<ol> <li>Always use <code>req()</code> to check inputs before processing</li> <li>Handle errors gracefully with <code>tryCatch()</code></li> <li>Provide user feedback with notifications and progress bars</li> <li>Use reactive values for data that changes</li> <li>Keep server functions organized and well-commented</li> <li>Test your app thoroughly with different inputs</li> <li>Use meaningful variable names and add comments</li> </ol>"},{"location":"shiny-apps/shiny-r/#common-debugging-tips","title":"Common Debugging Tips","text":"<ul> <li>Use <code>print()</code> or <code>cat()</code> statements to debug server logic</li> <li>Check the R console for error messages</li> <li>Use browser() to pause execution and inspect variables</li> <li>Test reactive expressions in isolation</li> <li>Ensure all required packages are loaded</li> </ul>"},{"location":"shiny-apps/shiny-r/#additional-resources","title":"Additional Resources","text":"<ul> <li>Shiny Gallery</li> <li>Shiny Application Layout Guide</li> </ul>"},{"location":"tools/container-tools/","title":"Using Scientific Tools with Singularity","text":"<p>Some container tools are installed as .sif files on Alma and some are on dockerhub, where they can be used either locally with docker, or on Alma via singularity. These examples have been built at the ICR in order to simplify the usage of the complex environments that are required for them.</p> <p>Note - singularity is only installed on the compute/interactive modes. It is not available on the login nodes.  </p>"},{"location":"tools/container-tools/#bcl-convert-from-illumina","title":"bcl-convert from illumina","text":"<p>The software is from illumina The image is on Alma, located at: /opt/software/containers/singularity/build/bcl.sif  </p> <p>To use it, you need to bind a path on scratch to the container. It will use that path to write out log files. The general command is (asking for the help):</p> <pre><code>singularity exec --bind /path/to/your/logs:/var/log/bcl-convert /opt/software/containers/singularity/build/bcl.sif bcl-convert --help\n</code></pre> <p>If I want to check it works for, call help and bind it to your home path: <pre><code>singularity exec --bind ~:/var/log/bcl-convert /opt/software/containers/singularity/build/bcl.sif bcl-convert --help\n</code></pre> Note the --bind argument is obligatory, the container has hard coded into it the requirement to pass in a path to /var/log/bcl-convert. If you don't do this you will get an error.</p>"},{"location":"tools/container-tools/#pvactools","title":"pvactools","text":"<p>pVACtools can be difficult to install and run on Alma so we have created a singularity image located in the shared singularity cache space. The software is built as a docker image but can be run through singularity on Alma. To get the help you can run: <pre><code>singularity exec /data/scratch/shared/SINGULARITY-DOWNLOAD/tools/.singularity/pvactools_latest.sif pvacseq run -h\n</code></pre></p> <p>An example to call the singularity image is, where you have created an out6 direcory and have your example data in pvacseq_example_data: <pre><code>singularity exec \\\n--bind ./out6:/out6 \\\n--bind ./pvacseq_example_data:/pvaseq_example_data \\\n/data/scratch/shared/SINGULARITY-DOWNLOAD/tools/.singularity/pvactools_latest.sif \\\npvacseq run pvacseq_example_data/annotated.expression.vcf.gz HCC1395_TUMOR_DNA HLA-A*29:02,HLA-B*45:01,DRB1*04:05 all out6 -e1 8,9,10 -e2 15\n</code></pre> Note the way that directories inside singularity have to first be declared and mapped with the <code>--bind</code> parameter.</p>"},{"location":"tools/container-tools/#pisca-from-the-icr","title":"pisca from the ICR","text":"<p>The image is on dockerhub, located at: docker://icrsc/pisca-run.  It was created for Heather Grant by the RSE team in Scientific Computing.  </p> <p>Pull it first into the directory you want to run it from, and then run it with a call that binds the directory in which you have the xml files to the container.  The general command is: <pre><code>singularity run -B /your/xml/dir:/mnt docker://icrsc/pisca-run your.xml\n</code></pre></p>"},{"location":"tools/mamba-tools/","title":"Using Scientific Tools with Mamba and Conda","text":"<p>This assumes that you have already followed the using mamba for the first time tutorial on this site.  </p> <p>Mamba environments manage dependencies and packages for you, and are the preferred way to install R packages. Most R packages can be installed from mamba by prepending \"r-\" eg \"r-tidyverse\" etc.  Only where the package does not exist is it recommended to use <code>package.install</code> in R.  </p> <p>Note - all of these installations assume you have logged on to an interactive session on Alma.  </p>"},{"location":"tools/mamba-tools/#ensembl-vep","title":"ensembl-vep","text":"<p>This is a package for variant effect prediction found on github. It is most commonly installed from github with perl but can be installed with conda.  </p> <p>When installed with conda, the package is called <code>ensembl-vep</code>.  The plugins are aliased as <code>vep_install</code>, below shows the creation of a conda environment and the installation of the human cache.  </p> <p><pre><code>mamba create --name bio-perl-vep -c conda-forge -c bioconda -c defaults perl-bioperl=1.7.8 ensembl-vep\nmamba activate bio-perl-vep\nvep -help\nvep_install -help\n</code></pre> Example usage: <pre><code>vep_install -a cf -s homo_sapiens -y GRCh38 -c ./ \u2014CONVERT\n\nvep --cache --dir_cache \"./\" \\\n   -i \"./C1D_filtered_SOPRANO.vcf\" \\\n   -o \"./vep_output.txt\" \\\n   --offline\n</code></pre> You must have the cache and dir-cache flags specified, and the human data needs to be in the dir-cache. There is no online access on Alma.  </p>"},{"location":"tools/mamba-tools/#mrcieutwosamplemr","title":"MRCIEU/TwoSampleMR","text":"<p>This is a package for Mendelian Randomization analysis found on github.  </p> <ul> <li>Create and activate a virtual environment <pre><code>mamba create --name mamba-TwoSampleMR -c bioconda samtools bcftools r-tidyverse r-remotes r-base=4.3\nmamba create --name mamba-TwoSampleMR -c bioconda r-tidyverse r-remotes r-base=4.3\nmamba activate mamba-TwoSampleMR\n</code></pre></li> <li>Manually install some packages in a dependency safe way <pre><code>mamba install r-meta\nmamba install r-nloptr\nmamba install r-lme4\n</code></pre></li> <li>Install custom package <pre><code>R -e 'remotes::install_github(\"MRCIEU/TwoSampleMR\")'\n</code></pre></li> </ul>"},{"location":"tools/mamba-tools/#nextflow-latest-version","title":"Nextflow (latest version)","text":"<p>See further nextflow instructions here.  </p> <pre><code>mamba create --name mamba_nf -c bioconda nextflow\nmamba activate mamba_nf\nnextflow -v\n</code></pre>"},{"location":"tools/overview/","title":"Scientific Software Tools","text":"<p>There is a vast array of scientific software written by researchers and developers around the world.  This software is often written in a variety of languages, including C, C++, Fortran, perl, R and Python.  There are a variety of ways these tools can be installed and used and this can be frustrating and time-consuming. Here we have instructions for the use of some common tools and some general advice on how to install and use scientific software.</p>"},{"location":"tools/overview/#should-i-use-the-software","title":"Should I use the software?","text":"<p>When evaluating scientific software pay attention to the quality of the software before you use it.  Some things to consider are:</p> <ul> <li>When was it last updated  </li> <li>How many people are using it  </li> <li>Are there any known issues  </li> <li>Is there a community of users who can help you  </li> <li>Is there a manual or other documentation  </li> <li>Is there a license  </li> </ul> <p>If the software is not well maintained, has no documentation, or has no community of users, you may want to consider using a different tool.</p> <p>Software is often distributed via GitHub and if so you can look at the issues and dates on the files, paper references and get a feel for it. It may be software from a major lab or company and so you can be more confident in its use.</p>"},{"location":"tools/overview/#how-to-install-software","title":"How to install software","text":"<p>There are a variety of ways to install scientific software, including: - Using conda or mamba (recommended) - Using python virtual environments where the software can be \"pip installed\" - Using \"module load\" - Using containers (singularity) either built by the ICR to facilitate use, or by the provider.  </p> <p>When evaluating software: work out how best to install it, and if you need guidance ask for help from scientific computing. There are an assortment of tools that have been asked about frequently that we have explicit instructions for, and these are listed below.</p>"},{"location":"tools/overview/#mamba-and-conda","title":"Mamba and Conda","text":"<ul> <li>ensemble-vep</li> <li>nextflow latest version</li> <li>MRCIEU/TwoSampleMR</li> </ul>"},{"location":"tools/overview/#singularity","title":"Singularity","text":"<ul> <li>bcl-convert from illumina</li> <li>pisca from the ICR</li> </ul>"},{"location":"training/nextflow/","title":"Nextflow","text":"<p>The ICR runs Nextflow community training following Nextflow's own schedule and videos. The material for those are internally available for ICR members in the ICR Institution's GitHub account. If you are an ICR member and would like access to this material, please contact the ICR's software group.</p> <p>Nextflow community training March 2024</p>"},{"location":"training/overview/","title":"Training from Scientific Computing and the Software Group","text":"<p>Training is run online and in person and can be found on the training sit on nexus.</p>"},{"location":"vcs/github/","title":"The ICR's Enterprise GitHub Account","text":"<p>The ICR has an enterprise GitHub account. This is free under academic license and allows us to have private repositories. This is useful for storing code that is not yet ready for public release, or for storing code that is proprietary to the ICR. It also allows us to have documentation and wikis for our repositories and to provide a professional branding and allow collaboration with external colleagues.</p> <p>The main ICR GitHub account is here: https://github.com/instituteofcancerresearch </p> <p>Groups may have their own individual accounts, for example the https://github.com/oncogenetics </p> <p>If you wish to have access to the ICR GitHub, contact the Service Hub and ask for the request to be sent to the Research Software Engineering team in Scientific Computing.</p> <p>ICR staff members can apply for the academic license for GitHub by following the instructions academic github application.  </p> <ul> <li>Students qualify for the student license  </li> <li>Other staff members qualify for the Teacher license which includes researchers: </li> </ul> <p>This has a number of benefits such as CoPilot in VSCode.</p> <p>Note on pushing code to GitHub if using command line Git</p> <p>From August 13, 2021, GitHub removed authentication with username and password. Now, if you want to push you changes to GitHub for the first time, you need to type in your GitHub username and personal access token as a password. Personal access tokens allow access necessary for the use case and are generated per project. More on personal access tokens and how to generate them can be found here: Managing your personal access tokens</p>"},{"location":"vcs/gitlab/","title":"ICR GitLab Installation","text":"<p>The ICR has an internal GitLab installation hosted at: https://git.icr.ac.uk/. This is internal to the ICR, accessible only on site or on the VPN.</p>"},{"location":"vcs/gitlab/#setting-up-an-ssh-key","title":"Setting up an ssh key","text":"<p>You may want access to the GitLab server from your local machine, or from Alma. To do this you will need to set up an ssh key and the instructions that follow are suitable for both Alma or your local machine, and will need to be done on both if you wish to access repositories from them.</p>"},{"location":"vcs/gitlab/#step-1-generate-a-new-ssh-key-if-you-need-one","title":"Step 1: Generate a new ssh key, if you need one","text":"<p><pre><code>cd ~/.ssh\ncat id_rsa.pub\n# if it already exists you can skip the generation and go to Step 2\nssh-keygen -t rsa -C \"edit-your-user-name@icr.ac.uk\"\n# To the questions that follow, just press enter to leave the responses blank\nEnter file in which to save the key (/home/demo/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again: \n</code></pre> There will be an output then along the lines: <pre><code>Your identification has been saved in ~/.ssh/id_rsa.\nYour public key has been saved in ~/.ssh/id_rsa.pub.\nThe key fingerprint is:\n4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@a\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|          .oo.   |\n|         .  o.E  |\n|        + .  o   |\n|     . = = .     |\n|      = S = .    |\n|     o + = +     |\n|      . o + o .  |\n|           . o   |\n|                 |\n+-----------------+\n</code></pre></p>"},{"location":"vcs/gitlab/#step-2-paste-the-public-part-on-gitlab","title":"Step 2: Paste the public part on GitLab","text":"<p>You now need to copy the public key and paste it into GitLab. - Open the file <code>~/.ssh/id_rsa.pub</code> with a text editor and copy the contents (or cat ~/.ssh/id_rsa.pub). - Go to the GitLab website and navigate to your profile settings on the left. - In the left-hand menu, click on <code>SSH Keys</code> - Choose \"Add new key\" and paste the key into the box. - Click <code>Add Key</code> and you are done!.  </p> <p></p>"},{"location":"workflows/containers/","title":"Singularity and Docker","text":"<p>There are times when you only have access to either docker or singularity. For example, when using Alma you only have access to singularity but not to docker. Also, when developing on your own workstation you can have access to docker only for instance.</p> <p>This article covers using docker images in both docker and singularity.</p>"},{"location":"workflows/containers/#developing-a-custom-docker-image","title":"Developing a custom docker image","text":"<p>If you develop a custom docker image and upload it to dockerhub, it will be available to pull from Docker in the standard way. An example of such an image managed by the ICR Software Group is: pisca-run</p> <p>Pull with docker or singularity <pre><code>docker pull icrsc/pisca-run\nsingularity pull docker://icrsc/pisca-run\n</code></pre></p> <p>The difference when pulling with Singularity is that you provide the docker URL.</p> <p>Docker command requires a path while singularity command requires a URL to the docker image. Generally, the docker URL consists of: docker:// preceding the image name.</p>"},{"location":"workflows/containers/#running-with-docker-or-singularity","title":"Running with Docker or Singularity","text":"<p>The Docker and Singularity run command runs a command in a new container, pulling the image if needed and starting/launching the container. Note that you can always choose to pull the image first, thus optimising the run command execution time.</p> <p>For this particular image: The docker command is: <code>docker run -v /your/xml/dir:/mnt icrsc/pisca-run your.xml</code> </p> <p>The singularity command is: <code>singularity run -B /your/xml/dir:/mnt docker://icrsc/pisca-run your.xml</code> </p> <p>There are slight differences in the parameters between singularity and docker.  For instance, passing a mounted directory is done using -v for docker and using -B for singularity.  Overall though, it is just the addition of docker:// in front of the image name.</p>"},{"location":"workflows/nextflow-envs/","title":"Nextflow in environments","text":"<p>The default Nextflow on Alma is only version 20 and this is unlikely to be sufficient for recent pipelines. Create an environment for your Nextflow pipelines using conda or mamba, or a python virtual environment.  </p> <p>If you load the mamba environment, you will have the correct java version for Nextflow. If you use a python virtual environment, you will need to load the correct java version yourself.  </p> <p>When using mamba or conda (they are interchangeable, be consistent - mamba is preferred),  make sure you have first followed the recommended way to ensure they are correctly initisalised on Alma:  mamba installation guide.</p> <p>The most recent Nextflow version in mamba as of May 2024 is <code>nextflow version 24.04.2.5914</code> </p>"},{"location":"workflows/nextflow-envs/#using-mamba-to-create-a-nextflow-environment","title":"Using mamba to create a Nextflow environment","text":"<pre><code>mamba create --name mamba_nf -c bioconda nextflow=24.04.4-0\nmamba activate mamba_nf\nnextflow -v\n</code></pre>"},{"location":"workflows/nextflow-envs/#using-conda-to-create-a-nextflow-environment","title":"Using conda to create a Nextflow environment","text":"<pre><code>conda create --name conda_nf -c bioconda nextflow\nconda activate conda_nf\nnextflow -v\n</code></pre>"},{"location":"workflows/nextflow-envs/#using-a-python-virtual-environment-to-create-a-nextflow-environment","title":"Using a python virtual environment to create a Nextflow environment","text":"<pre><code>python3 -m venv venv_nf\nsource venv_nf/bin/activate\npip install nextflow\nmodule load java/jdk15.0.1\nnextflow -v\n</code></pre>"},{"location":"workflows/nextflow/","title":"Nextflow vs slurm","text":"<p>This article compares a Nextflow workflow with a slurm workflow using a very simple so-called embarrassingly parallel workflow as an example.</p>"},{"location":"workflows/nextflow/#this-article-covers","title":"This article covers:","text":"<ul> <li>how to convert an existing slurm workflow to Nextflow</li> <li>a simple introduction to Nextflow</li> <li>a simple help to get Nextflow working on alma</li> <li>a simple introduction to running a slurm job on alma</li> </ul> <p>At the end of this you will have a simple dependent array job in slurm running on alma, alongside a Nextflow workflow that gives the same answer. Both workflows will call the same python scripts that are doing the real work.</p>"},{"location":"workflows/nextflow/#why-use-nextflow","title":"Why use Nextflow?","text":"<p>There are pros and cons to any method, and there are other workflow methods being used at the ICR. Some of the advantages of Nextflow are:</p> <ul> <li>adoption by the bioinformatics community to produce a catalogue of open-source, maintained pipelines at nf-core </li> <li>you can create re-usable modules</li> <li>it is platform independent, you can run on slurm or locally with very little change</li> <li>checkpointing is built in, so if something fails it can pick up where it left off</li> <li>the dependencies are sophisticated so you can build whatever you need</li> <li>there is seamless integration with docker and singularity</li> </ul>"},{"location":"workflows/nextflow/#what-does-this-workflow-do","title":"What does this workflow do?","text":"<p>The workflow is based on input-output of files and directories, consistent with the way many bioinformatics applications work.  This example will do the following:</p> <ul> <li>Using a python script, create files for the numbers 1:n - creating a file for each</li> <li>Once this is complete, separate processes take each of these files and square them, unnecessarily using numpy and sleeping for a user defined time to demo</li> <li>Once that is complete, all the results are added up and published to a results directory</li> <li>Links will be given for further information at various steps to keep the document simple.</li> </ul>"},{"location":"workflows/nextflow/#tutorial","title":"Tutorial","text":"<ol> <li>Ensure you have access to Alma If you are a new user to Alma you will need to request access.</li> </ol> <p>See above also for a reminder of how to access Alma, or some documents on slurm here.</p> <ol> <li>Check you anaconda environment You want to be running an updated anaconda environment which will ensure a recent python. If you have not already done this, then follow these instructions here. </li> </ol> <pre><code>$ python3 --version\nPython 3.8.3\n</code></pre> <ol> <li>Log on to an interactive/compute node Once at the Alma login prompt, access a compute node:</li> </ol> <p><pre><code>$ ssh username@alma.icr.ac.uk\n$ srun --pty -t 12:00:00 -p interactive bash\n</code></pre> The interactive nodes are required for running Nextflow as singularity is not installed on the login nodes. You do not need to load singularity, it is loaded by default. In this example we will not be using singularity, look at the nf-core training page for more details.</p> <ol> <li>Navigate to a good location Navigate to a good location where you will pull a GitHub repo. Create a directory that you might pull in a few repos for further training. Example shown for the scratch data in Scientific Computing.</li> </ol> <pre><code>$ cd /data/scratch/a/good/location/\n</code></pre> <p>5a. Running this session for the first time Clone the GitHub repo and then navigate to that directory. Then, you will need to make sure that python and Nextflow are installed in a virtual environment.</p> <p><pre><code>$ git clone git@github.com:ICR-RSE-Group/nextflow-toy.git\n$ cd nextflow-toy\n$ cd toy_submission_\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ chmod +x squaring_slurm.sh\n$ chmod +x squaring_nf.sh\n$ pip install --upgrade pip\n$ pip install nextflow\n$ pip install numpy\n\n5b. Returning to this session\nActivate the python environment. Navigate to the directory, and then:\n</code></pre> $ cd nextflow-toy $ cd toy-workflow $ source venv/bin/activate <pre><code>If you need to pull the latest version of the tutorial (and lose your changes):\n</code></pre> $ git checkout -- . $ git pull $ chmod +x squaring_slurm.sh $ chmod +x squaring_nf.sh <pre><code>6. Check everything is loaded and the right version\nLoad java and check python and nextflow to be sure they are loaded\n</code></pre> $ module load java/jdk15.0.1 $ nextflow -v $ python --version <pre><code>You might want to open the github repo so you can browse the code as you go along.\n\n7. The old way - a slurm workflow\nSubmit each 1 by 1\n</code></pre> $ sbatch squaring_1_split.sh $ sbatch --dependency=afterok:01234 bin/squaring_2_square.sh $ sbatch --dependency=afterok:56789 bin/squaring_3_gather.sh <pre><code>Or submit as a workflow\n</code></pre> $ ./squaring_slurm.sh <pre><code>1-by-1 requires you to type in the job dependency each time, but is simpler if you are testing 1 step. The batch submits them as dependent batches.\n\n8. How do you know if slurm worked?\nWhen it has finished there will be a results file in the results/slurm_total.txt Each step is saving its file in work_slurm so you can see the intermediate results there.\n\n9. Monitoring and managing the slurm queues\nTo check if your jobs have been successfully submitted and to get an idea of their status you can monitor the slurm queues with:\n`$ squeue -u username`\nYou can cancel them using:\n`$ scancel 12345`\n\n10. Running this same workflow with nextflow\nRun it locally in a single process.\n</code></pre> $ nextflow run squaring_nf.nf</p>"},{"location":"workflows/nextflow/#or","title":"or","text":"<p>$ nextflow run squaring_nf.nf -resume <pre><code>The resume flag is where the checkpointing is handled - the first time there is nothing to resume, but after that it will pick up where it left off. Take off the resume flag if you want it to recalculate.\n\nThe sleep flag can be changed from the command line easily:\n</code></pre> $ nextflow run squaring_nf.nf --sleep 20 <pre><code>You will see in the workfolder the command.sh file now passes the parameter as 20.\n\nThe configuration of nextflow batches  is complex, with precendence being given to command-line parameters first.\n\n11. Has it worked?\nEach step of the calculation creates a new file: split creates a file for each \"n\" which you will find in the work directories. Every nextflow job has its own folder in \"work\" where it has all the data it needs to run.\n\nFinally the data is published to results/nextflow_total.txt\n\n12. Running nextflow on slurm\nYou can run the entire thing on slurm with hardly any change.\n</code></pre> $ nextflow run squaring_nf.nf -process.executor slurm <pre><code>Simply specify the process executor to be slurm and it will magically be distributed across nodes.\n\n10. Slurming the nextflow controller\nYou may have noticed that while running the nextflow workflow, you had to be running the compute node actively. The final step is to submit the nextflow processor to slurm so the master-worker controller is managing the workflow. Some info on this here: master-worker nextflow \n\nFor this, a batch file is created to submit nextflow via sbatch.\n\n$ sbatch squaring_nf.sh\nYou know that it has worked when the file results/nextflow_total.txt is updated. Slurm has log files too, and this has been specified in the header of the slurm batch in the parameter --output\n</code></pre></p>"},{"location":"workflows/nextflow/#binbash","title":"!/bin/bash","text":""},{"location":"workflows/nextflow/#sbatch-job-namenf","title":"SBATCH --job-name=nf","text":""},{"location":"workflows/nextflow/#sbatch-outputwork_slurmnftxt","title":"SBATCH --output=work_slurm/nf.txt","text":""},{"location":"workflows/nextflow/#sbatch-partitionmaster-worker","title":"SBATCH --partition=master-worker","text":""},{"location":"workflows/nextflow/#sbatch-ntasks1","title":"SBATCH --ntasks=1","text":""},{"location":"workflows/nextflow/#sbatch-time10000","title":"SBATCH --time=1:00:00","text":""},{"location":"workflows/nextflow/#sbatch-mem-per-cpu1000","title":"SBATCH --mem-per-cpu=1000","text":"<p>srun nextflow run squaring_nf.nf -process.executor slurm ```</p> <p>Look in work_slurm/nf.txt and you will see that it contains what was in the command prompt before, it will keep updating with the queue status until complete where it will log some basic stats.</p> <p>NOTE It is safe to empty the work folder every time you rerun nextflow if you are interested in analysing what is going on. I recommend spending sometime looking at the contents of the work directry and how it matches up with the scripts.</p> <p>The workflow is used for \"resume\" though so it is not safe if you want to re-run with the resume flag.</p> <p>Continue with your nextflow learning You can continue your nextflow learning at nextflow, with their tutorial basic training.</p>"},{"location":"workflows/nf-core-1/","title":"Running nf-core on Alma - Setup and check","text":"<p>This tutorial covers running a simple test nf-core workflow on Alma to sanity check access and modules are installed correctly and working and run a test. In part 2 run sarek.</p>"},{"location":"workflows/nf-core-1/#step-1-log-on-to-an-interactive-node-on-alma","title":"Step 1: Log on to an interactive node on Alma","text":"<pre><code># to alma\nssh username@alma.icr.ac.uk\n# interactive session with 10GB memory and 2 cores\nsrun --pty --mem=10GB -c 2 -t 30:00:00 -p interactive bash    \n</code></pre>"},{"location":"workflows/nf-core-1/#before-starting","title":"Before starting","text":"<p>You should have initialised mamba as part of your standard Alma setup. If you have, you will see this by default python. <code>/opt/software/easybuild/software/Mamba/23.1.0-0/bin/python</code> Check it by the following: <pre><code>(base) [ralcraft@node01(alma)]$ which python\n/opt/software/easybuild/software/Mamba/23.1.0-0/bin/python\n(base) [ralcraft@node01(alma)]$ python --version\nPython 3.10.10\n</code></pre> If this is not what you see, follow the instructions for initialising a mamba environment here </p>"},{"location":"workflows/nf-core-1/#step-2-create-an-nf-core-directory","title":"Step 2: Create an nf-core directory","text":"<pre><code>cd /data/scratch/YOUR/PATH/GROUP/username/\nmkdir nf-core\ncd nf-core\n</code></pre>"},{"location":"workflows/nf-core-1/#step-3-install-the-nf-coretools-package-and-python-environment","title":"Step 3: Install the nf-core/tools package and python environment","text":"<p>You only need to install the python environment once, subsequently you can load it. If you are already in a python environment type <code>deactivate</code> to exit it.</p> <ul> <li> <p>Creating the python environment <pre><code>python -m venv .venv\nsource ./.venv/bin/activate\npip install --upgrade pip\npip install nextflow\npip install nf-core\n</code></pre></p> </li> <li> <p>Reloading it in subsequent sessions <pre><code>source ./.venv/bin/activate\n</code></pre></p> </li> <li>Note you could alternatively use a mamba environment for this. The mamba environment loads the java version required for nextflow. <pre><code>mamba create --name mamba_nf nextflow\nmamba activate mamba_nf\n</code></pre></li> </ul>"},{"location":"workflows/nf-core-1/#step-4-load-java-and-export-the-singularity-cache-location","title":"Step 4: Load java and export the singularity cache location","text":"<p>You will need to do this in every session <pre><code>module load java/jdk15.0.1\nexport NXF_SINGULARITY_CACHEDIR=/data/scratch/YOUR/PATH/GROUP/username/.singularity/cache\n</code></pre></p>"},{"location":"workflows/nf-core-1/#step-5-check-the-versions","title":"Step 5: Check the versions","text":"<pre><code>nextflow -v\nsingularity --version\nnf-core --version\n</code></pre>"},{"location":"workflows/nf-core-1/#step-6-run-the-nf-core-test-workflow","title":"Step 6: Run the nf-core test workflow","text":"<p>The nf-core workflows can be run in 3 ways, either directly from GitHub, or they can be downloaded first. Although it is recommended to run them directly from GitHub unless you are developing them, it can cause GitHub access problems in an HPC environment, so I will describe them, with a preference for the GitHub clone.</p> <ul> <li> <ol> <li>Running from GitHub <pre><code>mkdir testpipeline_run #the _run to distinguish it from when we manually pull\ncd testpipeline_run\nnextflow run nf-core/testpipeline -profile test,singularity --outdir my-outdir\n</code></pre></li> </ol> </li> <li> <ol> <li>Cloning from GitHub and running start back in the nfcore directory (<code>cd /data/scratch/YOUR/PATH/GROUP/username/nf-core</code> or <code>cd ..</code>) <pre><code>git clone git@github.com:nf-core/testpipeline.git\ncd testpipeline_run\nnextflow run main.nf -profile test,singularity --outdir my-outdir\n</code></pre></li> </ol> </li> <li> <ol> <li>Using nf-core download module With this option, you download the entire workflow so it can be run offline. With the other options there is downloading as and when so eventually everything will be downloaded - e.g., the singularity modules are downloaded on first use and then reused from the singularity cache. With download, the entire thing is downloaded into the given directory. This may not be desired. There can be problems with this functionality so go back to a previous mode if so.</li> </ol> </li> </ul> <p>For this you type in <code>nf-core download</code> and then follow the prompts: </p>"},{"location":"workflows/nf-core-1/#step-7-check-the-output","title":"Step 7: Check the output","text":"<p>The pipeline outputs are written in two newly created directories: - an output directory as specified in the command argument --outdir. - a work directory in the directory they were running from. The work directory is important for troubleshooting as any error will be in the logs there including all the data and parameters used in the commands that failed.</p>"},{"location":"workflows/nf-core-2/","title":"Running nf-core on Alma - Sarek: a real pipeline","text":"<p><code>Maintained by: Rachel Alcraft. Last updated: 12th June 2024</code></p> <p>This tutorial covers running a simple version of sarek with data downloaded from zenodo. This forms the basis of nf-core pipelines that you can build up from with your own data. The important aspect is ensuring you have this working on Alma as the configuration for memory and CPUs is quite specific.</p> <p>Note:  nf-core pipelines are not under our control and may change faster than this documentation. Please do tell us if you have any problems with this tutorial. Please don't assume it is \"just you\", or the documentation is not supposed to work - we really want it to help everyone and value everyone's input in helping us keep it up-to-date by reporting any problems with it. Send an email to schelpdesk with the title \"Alma Cookbook - Sarek tutorial problem\" and it will be filtered to the correct place.</p> <p>Important:  Everything in this tutorial is intentional in order to work on Alma so take care with each step \u263a\ufe0f Note that you need to have an interactive session with at least 30 GB and 2 cores to run the workflow interactively.</p>"},{"location":"workflows/nf-core-2/#step-1-log-on-and-activate-a-mamba-session","title":"Step 1: Log on and activate a mamba session","text":"<p>Note that mamba is the preferred environment, make sure you have followed this tutorial first: Nextflow with mamba.</p> <pre><code># to alma\nssh username@alma.icr.ac.uk\n# interactive session with 10GB memory and 2 cores\nsrun --pty --mem=30GB -c 2 -t 30:00:00 -p interactive bash    \n# navigate to a sensible place\ncd /data/scratch/YOUR/PATH/GROUP/username/somewhere/sensible\n# activate mamba\nmamba activate your-mamba-env\n# export the singularity cache variable to a sesnible place\nexport NXF_SINGULARITY_CACHEDIR=/data/scratch/YOUR/PATH/GROUP/username/.singularity/cache\n# check the version of nextflow is as you expect\nnf-core --version\n</code></pre>"},{"location":"workflows/nf-core-2/#step-2-run-a-real-pipeline-in-test","title":"Step 2: Run a real pipeline in test","text":"<p>You can run any nf-core pipeline in the same way, just replace testpipeline with the name of the pipeline you want to run. You can find the list of pipelines at nf-core. For this example we will run the nf-core/sarek pipeline.  We saw a few ways to use an nf-core pipeline in the test, for this we will clone the pipeline.</p> <p>Starting in the nf-core directory: <pre><code>git clone git@github.com:nf-core/sarek.git\ncd sarek\nnextflow run main.nf -profile test,singularity -resume --outdir my-outdir\n</code></pre></p> <p>Warning:  The most likely problem with this step will be pulling the singularity images. These can be large and timeout. If you have a problem with this, you can try to pull the images manually, or ask for help. To manually pull you want to use wget instead of singularity as it is quicker, and you navigate to the singularity cache directory (that you specified above) and wget the image from within there. Instructions  on how you turn a singularity error into a wget call are here. It could save time if you pull these common problematic images before you begin: <pre><code># navigate to your singularity cache as above\ncd /data/scratch/YOUR/PATH/GROUP/username/.singularity/cache\n# pull the few images I know to be troublesome:\n\nwget https://depot.galaxyproject.org/singularity/mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb:7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0 -O depot.galaxyproject.org-singularity-mulled-v2-d9e7bad0f7fbc8f4458d5c3ab7ffaaf0235b59fb-7cc3d06cbf42e28c5e2ebfc7c858654c7340a9d5-0.img\n\nwget https://depot.galaxyproject.org/singularity/gatk4:4.5.0.0--py36hdfd78af_0 -O depot.galaxyproject.org-singularity-gatk4-4.5.0.0--py36hdfd78af_0.img\n\nwget https://depot.galaxyproject.org/singularity/multiqc:1.21--pyhdfd78af_0 -O depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\n</code></pre></p>"},{"location":"workflows/nf-core-2/#step-3-run-a-real-pipeline-with-data","title":"Step 3: Run a real pipeline with data","text":"<p>Sarek pipeline</p> <p>We first need to generate some test data (if you don't have some). Create an inputs directory sarek/inputs.  </p> <p>Sample paired reads can be found on zenodo Download these and name them <code>T4A_R.fastq</code> and <code>T4A_F.fastq</code></p> <p>Move the files to the inputs directory within the sarek directory (I just copy and paste with the file system), and navigate there to compress the files, then navigate back:</p> <p><pre><code>cd /data/scratch/YOUR/PATH/GROUP/username/nf-core/sarek/inputs\ngzip T4A_R.fastq\ngzip T4A_F.fastq\ncd ..\n</code></pre> We need a samplesheet as an input that points to this sample data. The simplest sample data is a pair of fastq files.  Create a file in the sarek/inputs directory called samplesheet.csv and paste in this: <pre><code>patient,sample,lane,fastq_1,fastq_2\npatient1,test_sample,lane_1,inputs/T4A_R.fastq.gz,inputs/T4A_F.fastq.gz\n</code></pre></p> <p>Within the sarek directory run the pipeline with the following command, specifying the max_cpus as the ones we have requested an interactive session with (2) <pre><code>nextflow run main.nf --input inputs/samplesheet.csv -profile singularity -resume --outdir my-outdir --genome GATK.GRCh38 --max_cpus 2\n</code></pre></p>"},{"location":"workflows/nf-core-2/#step-4-running-sarek-on-slurm","title":"Step 4: Running sarek on slurm","text":"<p>The reason we are using an HPC cluster is to distribute this work across the cluster's compute nodes. To do this we can specify that we want to use slurm. We also need to pass in some alma specific memory parameters. To do this, create a file in the sarek/conf directory called icr.config, and put inside it this contents: <pre><code>singularity {\n  enabled = true\n  autoMounts = true\n}\n\nprocess {\n    queue=\"compute\"\n    executor = \"SLURM\"\n    shell  = ['/bin/bash', '-euo', 'pipefail']\n    // memory errors which should be retried. otherwise error out\n    errorStrategy = { task.exitStatus in [143,137,104,134,139,140,247] ? 'retry' : 'finish' }\n    maxRetries    = 1\n    maxErrors     = '-1'\n}\n\nexecutor {\n    perCpuMemAllocation = true\n}\n</code></pre></p> <p>The final section for the execution is important. It tells Nextflow to use the memory allocation per CPU that we have requested. This is important as the memory allocation is different on the cluster to the local machine. Notes about this are here.</p> <p>You can now run the pipeline using the slurm cluster with this command:</p> <pre><code>nextflow run main.nf --input inputs/samplesheet.csv -profile singularity -resume --outdir my-outdir --genome GATK.GRCh38 -c conf/icr.config\n</code></pre> <p>You will notice that this takes over your session. If you want to monitor the queues you can open another ssh session to Alma - you don't need to go an interactive session, you can stay on the login node, and simply type <pre><code>sacct\n</code></pre> This will give something like this, showing all the running processes on slurm: <pre><code>(base) [ralcraft@login(alma) ~]$ sacct\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n13777641           bash interacti+   infotech          2    RUNNING      0:0 \n13777641.ex+     extern              infotech          2    RUNNING      0:0 \n13777641.0         bash              infotech          2    RUNNING      0:0 \n13777644     nf-NFCORE+    compute   infotech         12  COMPLETED      0:0 \n13777644.ba+      batch              infotech         12  COMPLETED      0:0 \n</code></pre></p>"},{"location":"workflows/nf-core-2/#step-5-running-sarek-on-slurm-from-the-master-worker-partition","title":"Step 5: Running sarek on slurm from the master-worker partition","text":"<p>Nextflow has a master thread that is controlling all the other processes. Alma has a special queue for this called the master-worker queue, where long-running threads can control their child processes (the compute node has a time limit to stop long-running jobs).</p> <p>To use this queue, kick off the job above using \"sbatch\" and send it to this master-worker queue.</p> <p>a) Create a file in the sarek directory called <code>sarek-master.sh</code> and fill it with this contents: <pre><code>#!/bin/bash\n#SBATCH --job-name=nf-username\n#SBATCH --output=slurm_out.txt\n#SBATCH --partition=master-worker\n#SBATCH --ntasks=1\n#SBATCH --time=1:00:00\n#SBATCH --mem-per-cpu=4000\n\nsrun nextflow run main.nf --input inputs/samplesheet.csv -profile singularity -resume --outdir my-outdir --genome GATK.GRCh38 -c conf/icr.config\n</code></pre> Start this job with <pre><code>sbatch sarek-master.sh\n</code></pre> You can check it has submitted using <code>sacct</code></p> <p>The output that previously went to screen can be seen in the file we specified in the batch, in this case <code>slurm_out.txt</code>. This file is recreated each time it is restarted.  </p> <p>There is also the .nextflow.log - a history of these is kept and they are renamed 1,2,3 etc when a new process is started. The work and out-dir are the same as before, and if you find any errors you can match the process up against the directory:</p>"},{"location":"workflows/recipe-nf-epitome/","title":"Epi2me Labs Nextflow Pipeline","text":"<p>This document walks you through running Oxford Nanopore's test pipeline using Nextflow.  The pipeline can be run directly from github, and in this example we are looking at the basecalling workflow.</p> <p>It is worth noting that this example is using the GPUs on Alma and is therefore following a recipe in order for this to work specifically on Alma.  The solution given here is to use a dual-sbatch pattern to run Nextflow from a GPU.</p> <p>In this example mamba and conda are interchangeable, so stick with 1 or the other. I use mamba. If you need any help setting up mamba or conda, or you are not sure if you have it activated on Alma, please see the mamba installation guide.</p> <p>All the work below is done from the command-line remaining in your chosen directory on scratch.  </p>"},{"location":"workflows/recipe-nf-epitome/#1-work-on-an-interactive-node-in-a-suitable-directory-you-have-created","title":"1. Work on an interactive node in a suitable directory you have created.","text":"<pre><code>ssh username@alma.icr.ac.uk\nsrun --pty --mem=8GB -c 1 -t 30:00:00 -p interactive bash\ncd /data/scratch/your/path/\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#2-create-and-activate-a-mamba-environment","title":"2. Create and activate a mamba environment","text":"<pre><code>mamba create --name epi2me-labs -c bioconda -c conda-forge nextflow pytorch cuda\nmamba activate epi2me-labs\nnextflow -v # check that nextflow is installed\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#3-pull-the-nextflow-pipeline-from-github","title":"3. Pull the nextflow pipeline from github","text":"<pre><code>nextflow pull epi2me-labs/wf-basecalling\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#4-setup-the-test-data","title":"4. Setup the test data","text":"<pre><code>wget https://ont-exd-int-s3-euwst1-epi2me-labs.s3.amazonaws.com/wf-basecalling/wf-basecalling-demo.tar.gz\ntar -xzvf wf-basecalling-demo.tar.gz\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#5-create-the-first-of-2-batch-files-the-nextflow-call","title":"5. Create the first of 2 batch files: the Nextflow call","text":"<p>This script is the sbatch call that will call Nextflow from the GPU. Copy the contents below into a file that you name nextflow-basecalling.sh in your working directory.  </p> <pre><code>#!/usr/bin/env bash\n\nsource ~/.bashrc\n\nmamba activate epi2me-labs\nmodule load cuda/11.1\n\nnextflow run epi2me-labs/wf-basecalling \\\n-profile singularity \\\n--input wf-basecalling-demo/input \\\n--ref wf-basecalling-demo/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \\\n--dorado_ext pod5 \\\n--out_dir output \\\n--basecaller_cfg dna_r10.4.1_e8.2_400bps_hac@v4.1.0 \\\n--remora_cfg \"dna_r10.4.1_e8.2_400bps_hac@v4.1.0_5mCG_5hmCG@v2\"\n--cuda_device cuda:all\n\nmamba deactivate\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#6-create-the-second-of-2-batch-files-the-calling-script","title":"6. Create the second of 2 batch files: the calling script","text":"<p>This script will submit the work to a gpu. Copy the contents below into a file that you name epi2me-basecalling.sh in your working directory.  </p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name=epi2me\n#SBATCH --output=epi2me.out\n#SBATCH --error=epi2me.err\n#SBATCH --partition=gpu\n#SBATCH --ntasks=1\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=12\n#SBATCH --mem-per-cpu=4096\n#SBATCH --gres=gpu:2\n\nmodule load cuda/11.1\nsrun nextflow-basecalling.sh\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#7-make-sure-you-have-the-correct-permissions-on-the-files-to-execute","title":"7. Make sure you have the correct permissions on the files to execute","text":"<pre><code>chmod +x epi2me-basecalling.sh\nchmod +x nextflow-basecalling.sh\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#8-submit-the-job","title":"8. Submit the job","text":"<pre><code>sbatch epi2me-basecalling.sh\n</code></pre>"},{"location":"workflows/recipe-nf-epitome/#9-check-the-status-of-the-job","title":"9. Check the status of the job","text":"<p>You can monitor the queues with the following command:</p> <pre><code>squeue -u username\n</code></pre> <p>You can monitor the progress of the job by looking at the output and error files:</p> <pre><code>cat epi2me.out\ncat epi2me.err\ncat .nextflow.log\n</code></pre> <p>The epi2me.out file will give you a good idea of the progress of the job. A successful completion of this test job will look something like this:</p> <pre><code>[fc/77f55a] wf_dorado:dorado (3)           | 3 of 3 \u2714\n[e4/727b8d] wf_dorado:make_mmi             | 1 of 1 \u2714\n[df/b1bb31] wf_\u2026ado:align_and_qsFilter (3) | 3 of 3 \u2714\n[90/ab4420] wf_dorado:merge_pass_calls     | 1 of 1 \u2714\n[e9/edf56a] wf_dorado:merge_fail_calls     | 1 of 1 \u2714\n[a0/359e6d] getVersions                    | 1 of 1 \u2714\n[07/05f7fa] getParams                      | 1 of 1 \u2714\n[49/9a87e1] cram_cache                     | 1 of 1 \u2714\n[5b/358c65] bamstats (3)                   | 3 of 3 \u2714\n[81/3d7644] progressive_stats (3)          | 3 of 3 \u2714\n[c2/a4b052] makeReport (3)                 | 3 of 3 \u2714\n[0a/fe7211] output_last                    | 1 of 1 \u2714\n[79/0e009d] output_stream (8)              | 9 of 9 \u2714\nWaiting for file transfers to complete (2 files)\nCompleted at: 31-May-2024 12:38:41\nDuration    : 6m 41s\nCPU hours   : 1.0\nSucceeded   : 31\n</code></pre>"}]}